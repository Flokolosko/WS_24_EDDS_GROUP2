{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extrinsic Evaluation\n",
    "Post - Filter approach:\n",
    "\n",
    "- we use the sensitivity labels from our intrinsic evaluation \n",
    "  \n",
    "- in the post filter approach we rank the documents according to the  coordinate ascent algorithm optimizing towards normalized\n",
    "Discounted Cumulative Gain (nDCG)\n",
    "\n",
    "- for that we use predefined functions from https://github.com/rueycheng/CoordinateAscent/blob/master as the implementation was not mentioned in the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> the general workflow will be the same as in extrinsic_LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "bert_results = pd.read_csv(\"sensitivity_predictions_comparison.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Query  Document-UI  Document-Index Relevance1 Relevance2 Relevance3\n",
      "0      1     87097544           40626          d        NaN          d\n",
      "1      1     87153566           11852          n        NaN          n\n",
      "2      1     87157536           12693          d        NaN        NaN\n",
      "3      1     87157537           12694          d        NaN        NaN\n",
      "4      1     87184723           15450          n        NaN        NaN\n"
     ]
    }
   ],
   "source": [
    "judged_df = pd.read_csv(\"./data/judged.txt\", sep=\"\\t\", header=None,\n",
    "                        names=[\"Query\", \"Document-UI\", \"Document-Index\", \"Relevance1\", \"Relevance2\", \"Relevance3\"])\n",
    "# Verify the loaded DataFrame\n",
    "print(judged_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the query documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Query                                         query_text\n",
      "0      1   Are there adverse effects on lipids when prog...\n",
      "1      2   pathophysiology and treatment of disseminated...\n",
      "2      3   anticardiolipin and lupus anticoagulants, pat...\n",
      "3      4                    reviews on subdurals in elderly\n",
      "4      5   effectiveness of etidronate in treating hyper...\n"
     ]
    }
   ],
   "source": [
    "def parse_queries(file_path):\n",
    "    \"\"\"\n",
    "    Parses query files with the format:\n",
    "    .I <Query ID>\n",
    "    .B <Background>\n",
    "    .W <Query Text>\n",
    "    \"\"\"\n",
    "    query_list = []\n",
    "    current_query_id = None\n",
    "    current_query_text = None\n",
    "    \n",
    "    with open(file_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\".I\"):\n",
    "                if current_query_id is not None and current_query_text is not None:\n",
    "                    query_list.append({\"Query\": current_query_id, \"query_text\": current_query_text})\n",
    "                current_query_id = int(line.split()[1])  # Extract Query ID\n",
    "                current_query_text = None  # Reset query text\n",
    "            elif line.startswith(\".W\"):\n",
    "                current_query_text = \"\"  # Initialize query text\n",
    "            elif current_query_text is not None:\n",
    "                current_query_text += \" \" + line  # Append to query text\n",
    "        \n",
    "        # Append the last query\n",
    "        if current_query_id is not None and current_query_text is not None:\n",
    "            query_list.append({\"Query\": current_query_id, \"query_text\": current_query_text.strip()})\n",
    "    \n",
    "    return pd.DataFrame(query_list)\n",
    "\n",
    "# Parse queries from files\n",
    "queries1 = parse_queries(\"querys/Queries1.txt\")\n",
    "queries2 = parse_queries(\"querys/Queries2.txt\")\n",
    "\n",
    "# Combine the queries into a single DataFrame\n",
    "queries_df = pd.concat([queries1, queries2], ignore_index=True)\n",
    "\n",
    "# Verify the parsed queries\n",
    "print(queries_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assign relevance labels from the judged.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Query  sequential identifier  Relevance_total\n",
      "0      1                  40626                2\n",
      "1      1                  11852                0\n",
      "2      1                  12693                1\n",
      "3      1                  12694                1\n",
      "4      1                  15450                0\n"
     ]
    }
   ],
   "source": [
    "def compute_relevance(row):\n",
    "    # Count how many of the relevance columns contain 'd' (relevant)\n",
    "    return sum(1 for val in [row[\"Relevance1\"], row[\"Relevance2\"], row[\"Relevance3\"]] if val == \"d\")\n",
    "\n",
    "# Add a total relevance score to judged_df\n",
    "judged_df[\"Relevance_total\"] = judged_df.apply(compute_relevance, axis=1)\n",
    "\n",
    "# Keep only the necessary columns\n",
    "judged_df_cleaned = judged_df[[\"Query\", \"Document-Index\", \"Relevance_total\"]].rename(\n",
    "    columns={\"Document-Index\": \"sequential identifier\"}\n",
    ")\n",
    "\n",
    "# Verify the processed DataFrame\n",
    "print(judged_df_cleaned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sequential identifier                                     title_abstract  \\\n",
      "0                    126  Prospective study of liver function in childre...   \n",
      "1                    154  Postpartum thyroiditis--an underdiagnosed dise...   \n",
      "2                    223  Primary renal actinomycosis in the presence of...   \n",
      "3                    283  Clinical course of breast cancer patients with...   \n",
      "4                    300  Cardiac abnormalities in patients with diffuse...   \n",
      "\n",
      "   actual_sensitivity  predicted_sensitivity  Query  Relevance_total  \n",
      "0                   0                      0     36                0  \n",
      "1                   1                      1     76                0  \n",
      "2                   1                      1      8                0  \n",
      "3                   0                      0     22                1  \n",
      "4                   0                      0     40                0  \n"
     ]
    }
   ],
   "source": [
    "bert_with_relevance = pd.merge(bert_results, judged_df_cleaned, on=\"sequential identifier\", how=\"left\")\n",
    "\n",
    "# Verify the merged DataFrame\n",
    "print(bert_with_relevance.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sequential identifier                                     title_abstract  \\\n",
      "0                    126  Prospective study of liver function in childre...   \n",
      "1                    126  Prospective study of liver function in childre...   \n",
      "2                    154  Postpartum thyroiditis--an underdiagnosed dise...   \n",
      "3                    154  Postpartum thyroiditis--an underdiagnosed dise...   \n",
      "4                    223  Primary renal actinomycosis in the presence of...   \n",
      "\n",
      "   actual_sensitivity  predicted_sensitivity  Query  Relevance_total  \\\n",
      "0                   0                      0     36                0   \n",
      "1                   0                      0     36                0   \n",
      "2                   1                      1     76                0   \n",
      "3                   1                      1     76                0   \n",
      "4                   1                      1      8                0   \n",
      "\n",
      "                                          query_text  \n",
      "0   CAN DILANTIN or PHENOBARBITAL CAUSE ISOLOATED...  \n",
      "1   CAN DILANTIN or PHENOBARBITAL CAUSE ISOLOATED...  \n",
      "2   radiation induced thyroiditis, differential d...  \n",
      "3   radiation induced thyroiditis, differential d...  \n",
      "4   work-up of hypertension in patient with horse...  \n"
     ]
    }
   ],
   "source": [
    "bert_full = pd.merge(bert_with_relevance, queries_df, on=\"Query\", how=\"left\")\n",
    "\n",
    "# Verify the final merged DataFrame\n",
    "print(bert_full.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post - Filter approach\n",
    "- calculate bm25 and proximity count accordingly for the analys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m bm25 \u001b[38;5;241m=\u001b[39m BM25Okapi(documents)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Compute BM25 scores\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m bert_full[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbm25_score\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [\u001b[43mbm25\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m[i] \u001b[38;5;28;01mfor\u001b[39;00m i, query \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(queries)]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\rank_bm25.py:119\u001b[0m, in \u001b[0;36mBM25Okapi.get_scores\u001b[1;34m(self, query)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m query:\n\u001b[0;32m    118\u001b[0m     q_freq \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([(doc\u001b[38;5;241m.\u001b[39mget(q) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdoc_freqs])\n\u001b[1;32m--> 119\u001b[0m     score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midf\u001b[38;5;241m.\u001b[39mget(q) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m*\u001b[39m (q_freq \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk1 \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m\n\u001b[0;32m    120\u001b[0m                                        (q_freq \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk1 \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mb \u001b[38;5;241m*\u001b[39m doc_len \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgdl)))\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m score\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "# Tokenize documents and queries\n",
    "documents = [doc.split() for doc in bert_full[\"title_abstract\"]]\n",
    "queries = [query.split() for query in bert_full[\"query_text\"]]\n",
    "\n",
    "# Initialize BM25 model\n",
    "bm25 = BM25Okapi(documents)\n",
    "\n",
    "# Compute BM25 scores\n",
    "bert_full[\"bm25_score\"] = [bm25.get_scores(query)[i] for i, query in enumerate(queries)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proximity_count(query, doc, window=8):\n",
    "    \"\"\"\n",
    "    Count how many query terms appear together within a specified window size in the document.\n",
    "    \"\"\"\n",
    "    terms = query.split()\n",
    "    doc_terms = doc.split()\n",
    "    count = 0\n",
    "    for i in range(len(doc_terms) - window + 1):\n",
    "        window_terms = doc_terms[i:i + window]\n",
    "        if all(term in window_terms for term in terms):\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "bert_full[\"proximity_count\"] = bert_full.apply(\n",
    "    lambda row: proximity_count(row[\"query_text\"], row[\"title_abstract\"]),\n",
    "    axis=1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
