{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extrinsic Evaluation\n",
    "Post - Filter approach:\n",
    "\n",
    "- we use the sensitivity labels from our intrinsic evaluation \n",
    "  \n",
    "- in the post filter approach we rank the documents according to the  coordinate ascent algorithm optimizing towards normalized\n",
    "Discounted Cumulative Gain (nDCG)\n",
    "\n",
    "- for that we use predefined functions from https://github.com/rueycheng/CoordinateAscent/blob/master as the implementation was not mentioned in the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> the general workflow will be the same as in extrinsic_LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "bert_results = pd.read_csv(\"intermediate_results/sensitivity_predictions_comparison.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Query  Document-UI  Document-Index Relevance1 Relevance2 Relevance3\n",
      "0      1     87097544           40626          d        NaN          d\n",
      "1      1     87153566           11852          n        NaN          n\n",
      "2      1     87157536           12693          d        NaN        NaN\n",
      "3      1     87157537           12694          d        NaN        NaN\n",
      "4      1     87184723           15450          n        NaN        NaN\n"
     ]
    }
   ],
   "source": [
    "judged_df = pd.read_csv(\"./data/judged.txt\", sep=\"\\t\", header=None,\n",
    "                        names=[\"Query\", \"Document-UI\", \"Document-Index\", \"Relevance1\", \"Relevance2\", \"Relevance3\"])\n",
    "# Verify the loaded DataFrame\n",
    "print(judged_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get the query documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Query                                         query_text\n",
      "0      1   Are there adverse effects on lipids when prog...\n",
      "1      2   pathophysiology and treatment of disseminated...\n",
      "2      3   anticardiolipin and lupus anticoagulants, pat...\n",
      "3      4                    reviews on subdurals in elderly\n",
      "4      5   effectiveness of etidronate in treating hyper...\n"
     ]
    }
   ],
   "source": [
    "def parse_queries(file_path):\n",
    "    \"\"\"\n",
    "    Parses query files with the format:\n",
    "    .I <Query ID>\n",
    "    .B <Background>\n",
    "    .W <Query Text>\n",
    "    \"\"\"\n",
    "    query_list = []\n",
    "    current_query_id = None\n",
    "    current_query_text = None\n",
    "    \n",
    "    with open(file_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\".I\"):\n",
    "                if current_query_id is not None and current_query_text is not None:\n",
    "                    query_list.append({\"Query\": current_query_id, \"query_text\": current_query_text})\n",
    "                current_query_id = int(line.split()[1])  # Extract Query ID\n",
    "                current_query_text = None  # Reset query text\n",
    "            elif line.startswith(\".W\"):\n",
    "                current_query_text = \"\"  # Initialize query text\n",
    "            elif current_query_text is not None:\n",
    "                current_query_text += \" \" + line  # Append to query text\n",
    "        \n",
    "        # Append the last query\n",
    "        if current_query_id is not None and current_query_text is not None:\n",
    "            query_list.append({\"Query\": current_query_id, \"query_text\": current_query_text.strip()})\n",
    "    \n",
    "    return pd.DataFrame(query_list)\n",
    "\n",
    "# Parse queries from files\n",
    "queries1 = parse_queries(\"data/Queries1.txt\")\n",
    "queries2 = parse_queries(\"data/Queries2.txt\")\n",
    "\n",
    "# Combine the queries into a single DataFrame\n",
    "queries_df = pd.concat([queries1, queries2], ignore_index=True)\n",
    "\n",
    "# Verify the parsed queries\n",
    "print(queries_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assign relevance labels from the judged.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Query  sequential identifier  Relevance_total\n",
      "0      1                  40626                2\n",
      "1      1                  11852                0\n",
      "2      1                  12693                1\n",
      "3      1                  12694                1\n",
      "4      1                  15450                0\n"
     ]
    }
   ],
   "source": [
    "def compute_relevance(row):\n",
    "    # Count how many of the relevance columns contain 'd' (relevant)\n",
    "    return sum(1 for val in [row[\"Relevance1\"], row[\"Relevance2\"], row[\"Relevance3\"]] if val == \"d\")\n",
    "\n",
    "# Add a total relevance score to judged_df\n",
    "judged_df[\"Relevance_total\"] = judged_df.apply(compute_relevance, axis=1)\n",
    "\n",
    "# Keep only the necessary columns\n",
    "judged_df_cleaned = judged_df[[\"Query\", \"Document-Index\", \"Relevance_total\"]].rename(\n",
    "    columns={\"Document-Index\": \"sequential identifier\"}\n",
    ")\n",
    "\n",
    "# Verify the processed DataFrame\n",
    "print(judged_df_cleaned.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sequential identifier                                     title_abstract  \\\n",
      "0                    126  Prospective study of liver function in childre...   \n",
      "1                    154  Postpartum thyroiditis--an underdiagnosed dise...   \n",
      "2                    223  Primary renal actinomycosis in the presence of...   \n",
      "3                    283  Clinical course of breast cancer patients with...   \n",
      "4                    300  Cardiac abnormalities in patients with diffuse...   \n",
      "\n",
      "   actual_sensitivity  predicted_sensitivity  Query  Relevance_total  \n",
      "0                   0                      0     36                0  \n",
      "1                   1                      1     76                0  \n",
      "2                   1                      1      8                0  \n",
      "3                   0                      0     22                1  \n",
      "4                   0                      0     40                0  \n"
     ]
    }
   ],
   "source": [
    "bert_with_relevance = pd.merge(bert_results, judged_df_cleaned, on=\"sequential identifier\", how=\"left\")\n",
    "\n",
    "# Verify the merged DataFrame\n",
    "print(bert_with_relevance.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sequential identifier                                     title_abstract  \\\n",
      "0                    126  Prospective study of liver function in childre...   \n",
      "1                    126  Prospective study of liver function in childre...   \n",
      "2                    154  Postpartum thyroiditis--an underdiagnosed dise...   \n",
      "3                    154  Postpartum thyroiditis--an underdiagnosed dise...   \n",
      "4                    223  Primary renal actinomycosis in the presence of...   \n",
      "\n",
      "   actual_sensitivity  predicted_sensitivity  Query  Relevance_total  \\\n",
      "0                   0                      0     36                0   \n",
      "1                   0                      0     36                0   \n",
      "2                   1                      1     76                0   \n",
      "3                   1                      1     76                0   \n",
      "4                   1                      1      8                0   \n",
      "\n",
      "                                          query_text  \n",
      "0   CAN DILANTIN or PHENOBARBITAL CAUSE ISOLOATED...  \n",
      "1   CAN DILANTIN or PHENOBARBITAL CAUSE ISOLOATED...  \n",
      "2   radiation induced thyroiditis, differential d...  \n",
      "3   radiation induced thyroiditis, differential d...  \n",
      "4   work-up of hypertension in patient with horse...  \n"
     ]
    }
   ],
   "source": [
    "bert_full = pd.merge(bert_with_relevance, queries_df, on=\"Query\", how=\"left\")\n",
    "\n",
    "# Verify the final merged DataFrame\n",
    "print(bert_full.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post - Filter approach\n",
    "- calculate bm25 and proximity count accordingly for the analysis\n",
    "- as we already calculated the BM250 and the proximity score in the extrinsic evaluation of Logistic Regression, we are going to reuse that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predicted_with_scores = pd.read_csv(\"intermediate_results/test_predicted_withProximity.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sequential identifier                                     title_abstract  \\\n",
      "0                    126  Prospective study of liver function in childre...   \n",
      "1                    126  Prospective study of liver function in childre...   \n",
      "2                    126  Prospective study of liver function in childre...   \n",
      "3                    126  Prospective study of liver function in childre...   \n",
      "4                    154  Postpartum thyroiditis--an underdiagnosed dise...   \n",
      "\n",
      "   actual_sensitivity  predicted_sensitivity  Query  Relevance_total  \\\n",
      "0                   0                      0     36                0   \n",
      "1                   0                      0     36                0   \n",
      "2                   0                      0     36                0   \n",
      "3                   0                      0     36                0   \n",
      "4                   1                      1     76                0   \n",
      "\n",
      "                                          query_text  proximity_count_x  \\\n",
      "0   CAN DILANTIN or PHENOBARBITAL CAUSE ISOLOATED...                  0   \n",
      "1   CAN DILANTIN or PHENOBARBITAL CAUSE ISOLOATED...                  0   \n",
      "2   CAN DILANTIN or PHENOBARBITAL CAUSE ISOLOATED...                  0   \n",
      "3   CAN DILANTIN or PHENOBARBITAL CAUSE ISOLOATED...                  0   \n",
      "4   radiation induced thyroiditis, differential d...                  0   \n",
      "\n",
      "   bm25_score  proximity_count_y  \n",
      "0   15.999363                  8  \n",
      "1   15.999363                  8  \n",
      "2   15.999363                  8  \n",
      "3   15.999363                  8  \n",
      "4    0.000000                  0  \n"
     ]
    }
   ],
   "source": [
    "bert_full_with_bm25 = pd.merge(\n",
    "    bert_full, \n",
    "    test_predicted_with_scores [[\"sequential identifier\", \"bm25_score\",\"proximity_count\"]],\n",
    "    on=\"sequential identifier\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Verify the merged DataFrame\n",
    "print(bert_full_with_bm25.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [\"bm25_score\", \"proximity_count\"]\n",
    "X_test = bert_full_with_bm25[feature_columns].values\n",
    "\n",
    "# Define relevance labels and Query IDs\n",
    "y_test = bert_full_with_bm25[\"Relevance_total\"].values\n",
    "qid_test = bert_full_with_bm25[\"Query\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t1\t1\t0.14686533331255644\n",
      "2\t1\t1\t0.14686533331255644\n"
     ]
    }
   ],
   "source": [
    "from coordinate_ascent import CoordinateAscent\n",
    "from metrics import NDCGScorer\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "\n",
    "scorer = NDCGScorer(k=10, idcg_cache={})\n",
    "\n",
    "X_test_sparse = csr_matrix(X_test)\n",
    "\n",
    "model = CoordinateAscent(n_restarts=2, max_iter=25, verbose=True, scorer=scorer).fit(X_test_sparse, y_test, qid_test)\n",
    "\n",
    "pred = model.predict(X_test_sparse, qid_test)\n",
    "\n",
    "bert_full_with_bm25[\"predicted_scores\"] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_full_with_bm25_filtered= bert_full_with_bm25[bert_full_with_bm25[\"predicted_sensitivity\"] == 0]\n",
    "\n",
    "X_test_filtered = bert_full_with_bm25_filtered[[\"bm25_score\", \"proximity_count\"]].values\n",
    "y_test_filtered = bert_full_with_bm25_filtered[\"Relevance_total\"].values  # Relevance labels\n",
    "qid_test_filtered = bert_full_with_bm25_filtered[\"Query\"].values  # Query IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Postfilter Average nCS-DCG@10: 0.1444\n"
     ]
    }
   ],
   "source": [
    "ndcg_score = scorer(y_test_filtered, bert_full_with_bm25_filtered[\"predicted_scores\"].values, qid_test_filtered).mean()\n",
    "\n",
    "# Print the final score\n",
    "print(f\"Postfilter Average nCS-DCG@10: {ndcg_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> these results are way off and not reproducable for us as we do not have enough information in the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joint-approach\n",
    "- here we want to find a balanced result between sensitivity and relevance \n",
    "- for that we apply the penalty for sensitvity directly during the ranking process\n",
    "- we again use the features from above, but also take sensitivity directly into account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [\"bm25_score\", \"proximity_count\"]\n",
    "X_joint = bert_full_with_bm25[feature_columns].values\n",
    "\n",
    "y_joint = bert_full_with_bm25[[\"Relevance_total\", \"predicted_sensitivity\"]].values\n",
    "qid_joint = bert_full_with_bm25[\"Query\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_joint = csr_matrix(X_joint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in the paper it was mentioned that they applied a penalty of 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics import NDCGScorer\n",
    "\n",
    "class nCS_DCGScorer:\n",
    "    def __init__(self, y_sensitivity, k=10, sensitivity_penalty=12, idcg_cache={}):\n",
    "        self.y_sensitivity = y_sensitivity  # Store sensitivity labels\n",
    "        self.k = k\n",
    "        self.sensitivity_penalty = sensitivity_penalty\n",
    "        self.ndcg_scorer = NDCGScorer(k=k, idcg_cache=idcg_cache)\n",
    "\n",
    "    def __call__(self, y_relevance, pred, qid):\n",
    "        # Apply sensitivity penalty to predicted scores\n",
    "        penalized_pred = pred.copy()\n",
    "        penalized_pred[self.y_sensitivity == 1] -= self.sensitivity_penalty\n",
    "\n",
    "        # Compute nDCG@10 with penalized predictions\n",
    "        return self.ndcg_scorer(y_relevance, penalized_pred, qid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t1\t1\t0.1468821487688108\n",
      "2\t1\t1\t0.1468821487688108\n",
      "3\t1\t0\t0.1467969080124722\n",
      "3\t2\t1\t0.14689512113470213\n",
      "3\t1\t0\t0.14692537062626018\n",
      "3\t2\t1\t0.1470190111129167\n",
      "4\t1\t1\t0.1468821487688108\n",
      "5\t1\t0\t0.1467969080124722\n",
      "5\t2\t1\t0.14689512113470213\n",
      "5\t1\t0\t0.14692537062626018\n",
      "5\t2\t1\t0.1470190111129167\n"
     ]
    }
   ],
   "source": [
    "y_relevance = y_joint[:, 0]\n",
    "y_sensitivity = y_joint[:, 1]\n",
    "\n",
    "ncs_dcg_scorer = nCS_DCGScorer(y_sensitivity=y_sensitivity, k=10, sensitivity_penalty=12)\n",
    "\n",
    "model = CoordinateAscent(\n",
    "    n_restarts=5,\n",
    "    max_iter=50,\n",
    "    verbose=True,\n",
    "    scorer=ncs_dcg_scorer  \n",
    ").fit(X_joint, y_relevance, qid_joint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joint Approach - Average nCS-DCG@10: 0.1470\n"
     ]
    }
   ],
   "source": [
    "pred_joint = model.predict(X_joint, qid_joint)\n",
    "\n",
    "bert_full_with_bm25[\"joint_scores\"] = pred_joint\n",
    "\n",
    "average_ncs_dcg = ncs_dcg_scorer(y_relevance, pred_joint, qid_joint).mean()\n",
    "print(f\"Joint Approach - Average nCS-DCG@10: {average_ncs_dcg:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "these values are way below the results from the paper\n",
    "-> as we dont have the exact approach they used in the paper, we cant reproduce the reults"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
