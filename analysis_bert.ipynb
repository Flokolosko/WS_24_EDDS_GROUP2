{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35dc65b72dbf70a2",
   "metadata": {},
   "source": [
    "# 1. Data loading\n",
    "Firstly, we will load the *OHSUMED* data from different files and merge them into one combined dataframe, containing all years from 1987 - 1991. Afterwards, we will load the file where the relevance labels of documents during all five years will be labeled. In the end, we will left join the relevance labels to our main dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e74b25fa83a4bba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T00:23:15.994221Z",
     "start_time": "2025-01-03T00:23:06.733592Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DistilBertTokenizer\n",
    "from transformers import DistilBertForSequenceClassification\n",
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "def parse_ohsumed_file(file_path):\n",
    "    \"\"\"Parses an OHSUMED file into a DataFrame with proper column names.\"\"\"\n",
    "    documents = []\n",
    "    document = {}\n",
    "\n",
    "    # Read the file line by line\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()  # Remove extra whitespace\n",
    "\n",
    "            if line.startswith(\".I\"):  # New document identifier\n",
    "                if document:  # If there's an existing document, save it\n",
    "                    documents.append(document)\n",
    "                document = {\"sequential identifier\": line[3:]}  # Initialize a new document\n",
    "\n",
    "            elif line.startswith(\".U\"):  # MEDLINE identifier\n",
    "                document[\"MEDLINE identifier\"] = next(f).strip()\n",
    "\n",
    "            elif line.startswith(\".S\"):  # Source\n",
    "                document[\"source\"] = next(f).strip()\n",
    "\n",
    "            elif line.startswith(\".M\"):  # MeSH terms\n",
    "                document[\"mesh_terms\"] = next(f).strip()\n",
    "\n",
    "            elif line.startswith(\".T\"):  # Title\n",
    "                document[\"title\"] = next(f).strip()\n",
    "\n",
    "            elif line.startswith(\".P\"):  # Publication type\n",
    "                document[\"publication type\"] = next(f).strip()\n",
    "\n",
    "            elif line.startswith(\".W\"):  # Abstract\n",
    "                document[\"abstract\"] = next(f).strip()\n",
    "\n",
    "            elif line.startswith(\".A\"):  # Author\n",
    "                document[\"author\"] = next(f).strip()\n",
    "\n",
    "    # Add the last document if it exists\n",
    "    if document:\n",
    "        documents.append(document)\n",
    "\n",
    "    # Convert the list of documents into a DataFrame\n",
    "    return pd.DataFrame(documents)\n",
    "\n",
    "# File paths for individual files\n",
    "file_87_path = \"./data/ohsumed.87.txt\"\n",
    "file_88_path = \"./data/ohsumed.88.txt\"\n",
    "file_89_path = \"./data/ohsumed.89.txt\"\n",
    "file_90_path = \"./data/ohsumed.90.txt\"\n",
    "file_91_path = \"./data/ohsumed.91.txt\"\n",
    "\n",
    "# Parse each file into its own DataFrame\n",
    "df_ohsumed_87 = parse_ohsumed_file(file_87_path)\n",
    "df_ohsumed_88 = parse_ohsumed_file(file_88_path)\n",
    "df_ohsumed_89 = parse_ohsumed_file(file_89_path)\n",
    "df_ohsumed_90 = parse_ohsumed_file(file_90_path)\n",
    "df_ohsumed_91 = parse_ohsumed_file(file_91_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f82e80aca558d1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T00:23:16.293057Z",
     "start_time": "2025-01-03T00:23:16.282917Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in ohsumed.87: 54710\n",
      "Number of rows in ohsumed.88: 70825\n",
      "Number of rows in ohsumed.89: 74869\n",
      "Number of rows in ohsumed.90: 73824\n",
      "Number of rows in ohsumed.91: 74338\n"
     ]
    }
   ],
   "source": [
    "# Checking row amount of all years files\n",
    "print(f\"Number of rows in ohsumed.87: {len(df_ohsumed_87)}\")\n",
    "print(f\"Number of rows in ohsumed.88: {len(df_ohsumed_88)}\")\n",
    "print(f\"Number of rows in ohsumed.89: {len(df_ohsumed_89)}\")\n",
    "print(f\"Number of rows in ohsumed.90: {len(df_ohsumed_90)}\")\n",
    "print(f\"Number of rows in ohsumed.91: {len(df_ohsumed_91)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e40060215a7b92d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T00:23:16.358974Z",
     "start_time": "2025-01-03T00:23:16.303019Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined DataFrame shape: (348566, 8)\n",
      "  sequential identifier MEDLINE identifier                             source  \\\n",
      "0                     1           87049087    Am J Emerg Med 8703; 4(6):491-5   \n",
      "1                     2           87049088  Am J Emerg Med 8703; 4(6):496-500   \n",
      "2                     3           87049089    Am J Emerg Med 8703; 4(6):501-3   \n",
      "3                     4           87049090    Am J Emerg Med 8703; 4(6):504-6   \n",
      "4                     5           87049092    Am J Emerg Med 8703; 4(6):511-3   \n",
      "\n",
      "                                          mesh_terms  \\\n",
      "0  Allied Health Personnel/*; Electric Countersho...   \n",
      "1  Antidepressive Agents, Tricyclic/*PO; Arrhythm...   \n",
      "2  Adult; Aircraft/*; Altitude/*; Blood Gas Monit...   \n",
      "3  Adolescence; Adult; Aged; Blood Glucose/*ME; D...   \n",
      "4  Aged; Aged, 80 and over; Case Report; Female; ...   \n",
      "\n",
      "                                               title  publication type  \\\n",
      "0  Refibrillation managed by EMT-Ds: incidence an...  JOURNAL ARTICLE.   \n",
      "1  Tricyclic antidepressant overdose: emergency d...  JOURNAL ARTICLE.   \n",
      "2  Transconjunctival oxygen monitoring as a predi...  JOURNAL ARTICLE.   \n",
      "3  Serum glucose changes after administration of ...  JOURNAL ARTICLE.   \n",
      "4  Nasogastric intubation: morbidity in an asympt...  JOURNAL ARTICLE.   \n",
      "\n",
      "                                            abstract  \\\n",
      "0  Some patients converted from ventricular fibri...   \n",
      "1  There is controversy regarding the appropriate...   \n",
      "2  As the use of helicopters for air transport of...   \n",
      "3  A prospective clinical trial was conducted to ...   \n",
      "4  An unusual case of a misdirected nasogastric t...   \n",
      "\n",
      "                                              author  \n",
      "0                               Stults KR; Brown DD.  \n",
      "1                 Foulke GE; Albertson TE; Walby WF.  \n",
      "2  Shufflebarger C; Jehle D; Cottington E; Martin M.  \n",
      "3                                          Adler PM.  \n",
      "4                                   Gough D; Rust D.  \n"
     ]
    }
   ],
   "source": [
    "# Combine all DataFrames into a single DataFrame\n",
    "ohsumed_combined_df = pd.concat([df_ohsumed_87, df_ohsumed_88, df_ohsumed_89, df_ohsumed_90, df_ohsumed_91], ignore_index=True)\n",
    "\n",
    "# Print the combined DataFrame's shape (rows and columns). Should be: 348566\n",
    "print(f\"Combined DataFrame shape: {ohsumed_combined_df.shape}\")\n",
    "\n",
    "# Display the first few rows of the combined DataFrame\n",
    "print(ohsumed_combined_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1caf5d95e1d30c27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T00:23:16.421630Z",
     "start_time": "2025-01-03T00:23:16.382103Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded judged file with 16140 rows\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Document-UI</th>\n",
       "      <th>Document-Index</th>\n",
       "      <th>Relevance1</th>\n",
       "      <th>Relevance2</th>\n",
       "      <th>Relevance3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>87097544</td>\n",
       "      <td>40626</td>\n",
       "      <td>d</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>87153566</td>\n",
       "      <td>11852</td>\n",
       "      <td>n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>87157536</td>\n",
       "      <td>12693</td>\n",
       "      <td>d</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>87157537</td>\n",
       "      <td>12694</td>\n",
       "      <td>d</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>87184723</td>\n",
       "      <td>15450</td>\n",
       "      <td>n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Query  Document-UI  Document-Index Relevance1 Relevance2 Relevance3\n",
       "0      1     87097544           40626          d        NaN          d\n",
       "1      1     87153566           11852          n        NaN          n\n",
       "2      1     87157536           12693          d        NaN        NaN\n",
       "3      1     87157537           12694          d        NaN        NaN\n",
       "4      1     87184723           15450          n        NaN        NaN"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the judged file (relevance labeles)\n",
    "judged_df = pd.read_csv(\"./data/judged.txt\", sep=\"\\t\", header=None,\n",
    "                        names=[\"Query\", \"Document-UI\", \"Document-Index\", \"Relevance1\", \"Relevance2\", \"Relevance3\"])\n",
    "\n",
    "print(f\"Loaded judged file with {len(judged_df)} rows\")\n",
    "judged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "716274c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique documents: 14430\n",
      "New df size: 14430\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Document-UI</th>\n",
       "      <th>Document-Index</th>\n",
       "      <th>Relevance1</th>\n",
       "      <th>Relevance2</th>\n",
       "      <th>Relevance3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>87097544</td>\n",
       "      <td>40626</td>\n",
       "      <td>d</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>87153566</td>\n",
       "      <td>11852</td>\n",
       "      <td>n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>87157536</td>\n",
       "      <td>12693</td>\n",
       "      <td>d</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>87157537</td>\n",
       "      <td>12694</td>\n",
       "      <td>d</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>87184723</td>\n",
       "      <td>15450</td>\n",
       "      <td>n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Query  Document-UI  Document-Index Relevance1 Relevance2 Relevance3\n",
       "0      1     87097544           40626          d        NaN          d\n",
       "1      1     87153566           11852          n        NaN          n\n",
       "2      1     87157536           12693          d        NaN        NaN\n",
       "3      1     87157537           12694          d        NaN        NaN\n",
       "4      1     87184723           15450          n        NaN        NaN"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get unique documents\n",
    "unique_docs = judged_df['Document-UI'].nunique()\n",
    "judged_df = judged_df.drop_duplicates(subset=['Document-UI'], keep='first')\n",
    "\n",
    "print(f\"Unique documents: {unique_docs}\")\n",
    "print(f\"New df size: {len(judged_df)}\")\n",
    "\n",
    "judged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fd89d53f138fcfc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T00:23:17.082326Z",
     "start_time": "2025-01-03T00:23:16.492793Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resulting DataFrame shape: (348566, 14)\n",
      "  sequential identifier  MEDLINE identifier  \\\n",
      "0                     1            87049087   \n",
      "1                     2            87049088   \n",
      "2                     3            87049089   \n",
      "3                     4            87049090   \n",
      "4                     5            87049092   \n",
      "\n",
      "                              source  \\\n",
      "0    Am J Emerg Med 8703; 4(6):491-5   \n",
      "1  Am J Emerg Med 8703; 4(6):496-500   \n",
      "2    Am J Emerg Med 8703; 4(6):501-3   \n",
      "3    Am J Emerg Med 8703; 4(6):504-6   \n",
      "4    Am J Emerg Med 8703; 4(6):511-3   \n",
      "\n",
      "                                          mesh_terms  \\\n",
      "0  Allied Health Personnel/*; Electric Countersho...   \n",
      "1  Antidepressive Agents, Tricyclic/*PO; Arrhythm...   \n",
      "2  Adult; Aircraft/*; Altitude/*; Blood Gas Monit...   \n",
      "3  Adolescence; Adult; Aged; Blood Glucose/*ME; D...   \n",
      "4  Aged; Aged, 80 and over; Case Report; Female; ...   \n",
      "\n",
      "                                               title  publication type  \\\n",
      "0  Refibrillation managed by EMT-Ds: incidence an...  JOURNAL ARTICLE.   \n",
      "1  Tricyclic antidepressant overdose: emergency d...  JOURNAL ARTICLE.   \n",
      "2  Transconjunctival oxygen monitoring as a predi...  JOURNAL ARTICLE.   \n",
      "3  Serum glucose changes after administration of ...  JOURNAL ARTICLE.   \n",
      "4  Nasogastric intubation: morbidity in an asympt...  JOURNAL ARTICLE.   \n",
      "\n",
      "                                            abstract  \\\n",
      "0  Some patients converted from ventricular fibri...   \n",
      "1  There is controversy regarding the appropriate...   \n",
      "2  As the use of helicopters for air transport of...   \n",
      "3  A prospective clinical trial was conducted to ...   \n",
      "4  An unusual case of a misdirected nasogastric t...   \n",
      "\n",
      "                                              author  Query  Document-Index  \\\n",
      "0                               Stults KR; Brown DD.    NaN             NaN   \n",
      "1                 Foulke GE; Albertson TE; Walby WF.    NaN             NaN   \n",
      "2  Shufflebarger C; Jehle D; Cottington E; Martin M.    NaN             NaN   \n",
      "3                                          Adler PM.    NaN             NaN   \n",
      "4                                   Gough D; Rust D.    NaN             NaN   \n",
      "\n",
      "  Relevance1 Relevance2 Relevance3  is_relevant_ind  \n",
      "0        NaN        NaN        NaN              NaN  \n",
      "1        NaN        NaN        NaN              NaN  \n",
      "2        NaN        NaN        NaN              NaN  \n",
      "3        NaN        NaN        NaN              NaN  \n",
      "4        NaN        NaN        NaN              NaN  \n",
      "Resulting DataFrame shape: (348566, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequential identifier</th>\n",
       "      <th>MEDLINE identifier</th>\n",
       "      <th>source</th>\n",
       "      <th>mesh_terms</th>\n",
       "      <th>title</th>\n",
       "      <th>publication type</th>\n",
       "      <th>abstract</th>\n",
       "      <th>author</th>\n",
       "      <th>is_relevant_ind</th>\n",
       "      <th>Relevance_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>87049087</td>\n",
       "      <td>Am J Emerg Med 8703; 4(6):491-5</td>\n",
       "      <td>Allied Health Personnel/*; Electric Countersho...</td>\n",
       "      <td>Refibrillation managed by EMT-Ds: incidence an...</td>\n",
       "      <td>JOURNAL ARTICLE.</td>\n",
       "      <td>Some patients converted from ventricular fibri...</td>\n",
       "      <td>Stults KR; Brown DD.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>87049088</td>\n",
       "      <td>Am J Emerg Med 8703; 4(6):496-500</td>\n",
       "      <td>Antidepressive Agents, Tricyclic/*PO; Arrhythm...</td>\n",
       "      <td>Tricyclic antidepressant overdose: emergency d...</td>\n",
       "      <td>JOURNAL ARTICLE.</td>\n",
       "      <td>There is controversy regarding the appropriate...</td>\n",
       "      <td>Foulke GE; Albertson TE; Walby WF.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>87049089</td>\n",
       "      <td>Am J Emerg Med 8703; 4(6):501-3</td>\n",
       "      <td>Adult; Aircraft/*; Altitude/*; Blood Gas Monit...</td>\n",
       "      <td>Transconjunctival oxygen monitoring as a predi...</td>\n",
       "      <td>JOURNAL ARTICLE.</td>\n",
       "      <td>As the use of helicopters for air transport of...</td>\n",
       "      <td>Shufflebarger C; Jehle D; Cottington E; Martin M.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>87049090</td>\n",
       "      <td>Am J Emerg Med 8703; 4(6):504-6</td>\n",
       "      <td>Adolescence; Adult; Aged; Blood Glucose/*ME; D...</td>\n",
       "      <td>Serum glucose changes after administration of ...</td>\n",
       "      <td>JOURNAL ARTICLE.</td>\n",
       "      <td>A prospective clinical trial was conducted to ...</td>\n",
       "      <td>Adler PM.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>87049092</td>\n",
       "      <td>Am J Emerg Med 8703; 4(6):511-3</td>\n",
       "      <td>Aged; Aged, 80 and over; Case Report; Female; ...</td>\n",
       "      <td>Nasogastric intubation: morbidity in an asympt...</td>\n",
       "      <td>JOURNAL ARTICLE.</td>\n",
       "      <td>An unusual case of a misdirected nasogastric t...</td>\n",
       "      <td>Gough D; Rust D.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sequential identifier  MEDLINE identifier  \\\n",
       "0                     1            87049087   \n",
       "1                     2            87049088   \n",
       "2                     3            87049089   \n",
       "3                     4            87049090   \n",
       "4                     5            87049092   \n",
       "\n",
       "                              source  \\\n",
       "0    Am J Emerg Med 8703; 4(6):491-5   \n",
       "1  Am J Emerg Med 8703; 4(6):496-500   \n",
       "2    Am J Emerg Med 8703; 4(6):501-3   \n",
       "3    Am J Emerg Med 8703; 4(6):504-6   \n",
       "4    Am J Emerg Med 8703; 4(6):511-3   \n",
       "\n",
       "                                          mesh_terms  \\\n",
       "0  Allied Health Personnel/*; Electric Countersho...   \n",
       "1  Antidepressive Agents, Tricyclic/*PO; Arrhythm...   \n",
       "2  Adult; Aircraft/*; Altitude/*; Blood Gas Monit...   \n",
       "3  Adolescence; Adult; Aged; Blood Glucose/*ME; D...   \n",
       "4  Aged; Aged, 80 and over; Case Report; Female; ...   \n",
       "\n",
       "                                               title  publication type  \\\n",
       "0  Refibrillation managed by EMT-Ds: incidence an...  JOURNAL ARTICLE.   \n",
       "1  Tricyclic antidepressant overdose: emergency d...  JOURNAL ARTICLE.   \n",
       "2  Transconjunctival oxygen monitoring as a predi...  JOURNAL ARTICLE.   \n",
       "3  Serum glucose changes after administration of ...  JOURNAL ARTICLE.   \n",
       "4  Nasogastric intubation: morbidity in an asympt...  JOURNAL ARTICLE.   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Some patients converted from ventricular fibri...   \n",
       "1  There is controversy regarding the appropriate...   \n",
       "2  As the use of helicopters for air transport of...   \n",
       "3  A prospective clinical trial was conducted to ...   \n",
       "4  An unusual case of a misdirected nasogastric t...   \n",
       "\n",
       "                                              author  is_relevant_ind  \\\n",
       "0                               Stults KR; Brown DD.              NaN   \n",
       "1                 Foulke GE; Albertson TE; Walby WF.              NaN   \n",
       "2  Shufflebarger C; Jehle D; Cottington E; Martin M.              NaN   \n",
       "3                                          Adler PM.              NaN   \n",
       "4                                   Gough D; Rust D.              NaN   \n",
       "\n",
       "   Relevance_total  \n",
       "0              NaN  \n",
       "1              NaN  \n",
       "2              NaN  \n",
       "3              NaN  \n",
       "4              NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "judged_df.rename(columns={\"Document-UI\": \"MEDLINE identifier\"}, inplace=True)\n",
    "\n",
    "# Converting key to string\n",
    "ohsumed_combined_df[\"MEDLINE identifier\"] = ohsumed_combined_df[\"MEDLINE identifier\"].astype(int)\n",
    "judged_df[\"MEDLINE identifier\"] = judged_df[\"MEDLINE identifier\"].astype(int)\n",
    "judged_df[\"is_relevant_ind\"] = 1\n",
    "\n",
    "# Perform the left join\n",
    "merged_df = ohsumed_combined_df.merge(judged_df, on=\"MEDLINE identifier\", how=\"left\")\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(f\"Resulting DataFrame shape: {merged_df.shape}\")\n",
    "print(merged_df.head())\n",
    "\n",
    "# Create a new column Relevance_total based on the rules that can be kept showing relevance for all three relevance columns\n",
    "merged_df[\"Relevance_total\"] = np.where(\n",
    "    ~merged_df[\"Relevance1\"].isna(),  # If Relevance1 is not NaN, take it\n",
    "    merged_df[\"Relevance1\"],\n",
    "    np.where(\n",
    "        ~merged_df[\"Relevance2\"].isna(),  # Else if Relevance2 is not NaN, take it\n",
    "        merged_df[\"Relevance2\"],\n",
    "        merged_df[\"Relevance3\"]  # Else take Relevance3\n",
    "    )\n",
    ")\n",
    "merged_df.head(10000)\n",
    "\n",
    "# Drop the specified columns\n",
    "columns_to_drop = [\"Query\", \"Document-Index\", \"Relevance1\", \"Relevance2\", \"Relevance3\"]\n",
    "merged_df.drop(columns=columns_to_drop, inplace=True)\n",
    "# Mapping relevance labels to int\n",
    "relevance_mapping = {'n': 0, 'p': 1, 'd': 2}\n",
    "\n",
    "# Filling mising values in abstract so that BERT can be trained on strings\n",
    "merged_df[\"abstract\"] = merged_df[\"abstract\"].fillna(\"\")\n",
    "\n",
    "# Apply the mapping to the Relevance1 column\n",
    "merged_df[\"Relevance_total\"] = merged_df[\"Relevance_total\"].map(relevance_mapping)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(f\"Resulting DataFrame shape: {merged_df.shape}\")\n",
    "merged_df.head()\n",
    "\n",
    "#filtered_df = merged_df[merged_df[\"Relevance1\"].notna()]\n",
    "#\n",
    "## Display the filtered DataFrame\n",
    "#print(f\"Number of rows where Relevance1 is not NaN: {len(filtered_df)}\")\n",
    "#print(filtered_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8661ed6849b71fb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T00:23:17.158143Z",
     "start_time": "2025-01-03T00:23:17.137846Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows where is_relevant_ind = 1: 14430\n"
     ]
    }
   ],
   "source": [
    "#Checking how many relevant documents are present in merged DF. Expectation: 16140\n",
    "count_is_relevant = merged_df[merged_df[\"is_relevant_ind\"] == 1].shape[0]\n",
    "print(f\"Number of rows where is_relevant_ind = 1: {count_is_relevant}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bf3be64eba304f",
   "metadata": {},
   "source": [
    "Now, we have a merged dataframe, containing all document data and also the relevance labeling of the documents. Now we can proceed with splitting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T00:23:17.434333Z",
     "start_time": "2025-01-03T00:23:17.216823Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set size: 14430\n",
      "Training set size: 284015\n",
      "Validation set size: 50121\n"
     ]
    }
   ],
   "source": [
    "test = merged_df[merged_df[\"is_relevant_ind\"] == 1]\n",
    "remaining_rows = merged_df[merged_df[\"is_relevant_ind\"] != 1]\n",
    "training, validation = train_test_split(remaining_rows, test_size=0.15, random_state=42)\n",
    "\n",
    "print(f\"Test set size: {len(test)}\")\n",
    "print(f\"Training set size: {len(training)}\")\n",
    "print(f\"Validation set size: {len(validation)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1384fb8e2299c55",
   "metadata": {},
   "source": [
    "Training BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7295d700bce05d7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T00:23:17.818083Z",
     "start_time": "2025-01-03T00:23:17.473316Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\timgr\\AppData\\Local\\Temp\\ipykernel_6216\\4000273643.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test.loc[:, 'title_abstract'] = test['title'] + ' ' + test['abstract']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequential identifier</th>\n",
       "      <th>MEDLINE identifier</th>\n",
       "      <th>source</th>\n",
       "      <th>mesh_terms</th>\n",
       "      <th>title</th>\n",
       "      <th>publication type</th>\n",
       "      <th>abstract</th>\n",
       "      <th>author</th>\n",
       "      <th>is_relevant_ind</th>\n",
       "      <th>Relevance_total</th>\n",
       "      <th>title_abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33697</th>\n",
       "      <td>33698</td>\n",
       "      <td>87124329</td>\n",
       "      <td>Am Heart J 8705; 113(2 Pt 1):273-9</td>\n",
       "      <td>Aged; Comparative Study; Electrocardiography/*...</td>\n",
       "      <td>Non-Q wave myocardial infarction: recent chang...</td>\n",
       "      <td>JOURNAL ARTICLE.</td>\n",
       "      <td>A community-wide study of patients hospitalize...</td>\n",
       "      <td>Goldberg RJ; Gore JM; Alpert JS; Dalen JE.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Non-Q wave myocardial infarction: recent chang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245340</th>\n",
       "      <td>245341</td>\n",
       "      <td>90357619</td>\n",
       "      <td>Transplant Proc 9011; 22(4):1885-6</td>\n",
       "      <td>Antibodies, Anti-Idiotypic/*IM; Antibodies, Mo...</td>\n",
       "      <td>IgM-anti-IgG antibody as cause of positive B-c...</td>\n",
       "      <td>JOURNAL ARTICLE.</td>\n",
       "      <td></td>\n",
       "      <td>Terness P; Berteli AJ; Steinitz M; Mytillineos...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IgM-anti-IgG antibody as cause of positive B-c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306208</th>\n",
       "      <td>306209</td>\n",
       "      <td>91170056</td>\n",
       "      <td>J Appl Physiol 9106; 69(6):2091-6</td>\n",
       "      <td>Animal; Blood Pressure; Cardiac Output; Hemody...</td>\n",
       "      <td>Altered baroreflex function after tail suspens...</td>\n",
       "      <td>JOURNAL ARTICLE.</td>\n",
       "      <td>Experiments were performed on conscious chroni...</td>\n",
       "      <td>Brizzee BL; Walker BR.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Altered baroreflex function after tail suspens...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5057</th>\n",
       "      <td>5058</td>\n",
       "      <td>87097540</td>\n",
       "      <td>Am J Obstet Gynecol 8704; 156(1):52-6</td>\n",
       "      <td>Apgar Score; Cesarean Section/*; Delivery/*MT;...</td>\n",
       "      <td>Randomized management of the second nonvertex ...</td>\n",
       "      <td>JOURNAL ARTICLE.</td>\n",
       "      <td>Sixty twin deliveries after the thirty-fifth g...</td>\n",
       "      <td>Rabinovici J; Barkai G; Reichman B; Serr DM; M...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Randomized management of the second nonvertex ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120427</th>\n",
       "      <td>120428</td>\n",
       "      <td>88110706</td>\n",
       "      <td>Chest 8805; 93(2):294-8</td>\n",
       "      <td>Adult; Aged; Arteries/*; Bloodletting/*; Carbo...</td>\n",
       "      <td>Single arterial puncture vs arterial cannula f...</td>\n",
       "      <td>JOURNAL ARTICLE.</td>\n",
       "      <td>In an attempt to find the least invasive, safe...</td>\n",
       "      <td>Frye M; DiBenedetto R; Lain D; Morgan K.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single arterial puncture vs arterial cannula f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sequential identifier  MEDLINE identifier  \\\n",
       "33697                  33698            87124329   \n",
       "245340                245341            90357619   \n",
       "306208                306209            91170056   \n",
       "5057                    5058            87097540   \n",
       "120427                120428            88110706   \n",
       "\n",
       "                                       source  \\\n",
       "33697      Am Heart J 8705; 113(2 Pt 1):273-9   \n",
       "245340     Transplant Proc 9011; 22(4):1885-6   \n",
       "306208      J Appl Physiol 9106; 69(6):2091-6   \n",
       "5057    Am J Obstet Gynecol 8704; 156(1):52-6   \n",
       "120427                Chest 8805; 93(2):294-8   \n",
       "\n",
       "                                               mesh_terms  \\\n",
       "33697   Aged; Comparative Study; Electrocardiography/*...   \n",
       "245340  Antibodies, Anti-Idiotypic/*IM; Antibodies, Mo...   \n",
       "306208  Animal; Blood Pressure; Cardiac Output; Hemody...   \n",
       "5057    Apgar Score; Cesarean Section/*; Delivery/*MT;...   \n",
       "120427  Adult; Aged; Arteries/*; Bloodletting/*; Carbo...   \n",
       "\n",
       "                                                    title  publication type  \\\n",
       "33697   Non-Q wave myocardial infarction: recent chang...  JOURNAL ARTICLE.   \n",
       "245340  IgM-anti-IgG antibody as cause of positive B-c...  JOURNAL ARTICLE.   \n",
       "306208  Altered baroreflex function after tail suspens...  JOURNAL ARTICLE.   \n",
       "5057    Randomized management of the second nonvertex ...  JOURNAL ARTICLE.   \n",
       "120427  Single arterial puncture vs arterial cannula f...  JOURNAL ARTICLE.   \n",
       "\n",
       "                                                 abstract  \\\n",
       "33697   A community-wide study of patients hospitalize...   \n",
       "245340                                                      \n",
       "306208  Experiments were performed on conscious chroni...   \n",
       "5057    Sixty twin deliveries after the thirty-fifth g...   \n",
       "120427  In an attempt to find the least invasive, safe...   \n",
       "\n",
       "                                                   author  is_relevant_ind  \\\n",
       "33697          Goldberg RJ; Gore JM; Alpert JS; Dalen JE.              NaN   \n",
       "245340  Terness P; Berteli AJ; Steinitz M; Mytillineos...              NaN   \n",
       "306208                             Brizzee BL; Walker BR.              NaN   \n",
       "5057    Rabinovici J; Barkai G; Reichman B; Serr DM; M...              NaN   \n",
       "120427           Frye M; DiBenedetto R; Lain D; Morgan K.              NaN   \n",
       "\n",
       "        Relevance_total                                     title_abstract  \n",
       "33697               NaN  Non-Q wave myocardial infarction: recent chang...  \n",
       "245340              NaN  IgM-anti-IgG antibody as cause of positive B-c...  \n",
       "306208              NaN  Altered baroreflex function after tail suspens...  \n",
       "5057                NaN  Randomized management of the second nonvertex ...  \n",
       "120427              NaN  Single arterial puncture vs arterial cannula f...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.loc[:, 'title_abstract'] = training['title'] + ' ' + training['abstract']\n",
    "test.loc[:, 'title_abstract'] = test['title'] + ' ' + test['abstract']\n",
    "validation.loc[:, 'title_abstract'] = validation['title'] + ' ' + validation['abstract']\n",
    "training.head()\n",
    "test.head()\n",
    "validation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "966286ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\timgr\\AppData\\Local\\Temp\\ipykernel_6216\\3943537723.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, \"processed_mesh_terms\"] = df[\"mesh_terms\"].apply(preprocess_mesh_terms)\n",
      "C:\\Users\\timgr\\AppData\\Local\\Temp\\ipykernel_6216\\3943537723.py:68: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, \"sensitive\"] = df[\"processed_mesh_terms\"].apply(is_sensitive_regex)\n",
      "C:\\Users\\timgr\\AppData\\Local\\Temp\\ipykernel_6216\\3943537723.py:69: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, \"label\"] = df[\"sensitive\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset: 8.1%\n",
      "Judged documents: 12.3%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\timgr\\AppData\\Local\\Temp\\ipykernel_6216\\3943537723.py:82: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.drop(columns=[\"sensitive\"], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Step 1: Parse the .bin file to extract sensitive MeSH terms\n",
    "def parse_mesh_bin(file_path, target_categories):\n",
    "    \"\"\"\n",
    "    Extract MeSH terms under specific categories from a .bin file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the .bin file.\n",
    "        target_categories (list): List of categories (e.g., [\"C12\", \"C13\"]).\n",
    "\n",
    "    Returns:\n",
    "        list: List of MeSH terms under the target categories.\n",
    "    \"\"\"\n",
    "    mesh_terms = []\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            # Split by ';' to separate terms and categories\n",
    "            parts = line.strip().split(\";\")\n",
    "            if len(parts) > 1:\n",
    "                term, category = parts[0].strip().lower(), parts[1].strip()\n",
    "                # Include terms under target categories\n",
    "                if any(category.startswith(target) for target in target_categories):\n",
    "                    mesh_terms.append(term)\n",
    "    return mesh_terms\n",
    "\n",
    "# Path to the .bin file\n",
    "file_path = \"mtrees2019.bin\"\n",
    "\n",
    "# Extract MeSH terms under C12 and C13\n",
    "sensitive_terms = parse_mesh_bin(file_path, [\"C12\", \"C13\"])\n",
    "\n",
    "# Step 2: Preprocessing function for the `mesh_terms` column\n",
    "def preprocess_mesh_terms(mesh_terms):\n",
    "    \"\"\"\n",
    "    Normalize and preprocess the MeSH terms in a document.\n",
    "\n",
    "    Args:\n",
    "        mesh_terms (str): The raw MeSH terms for a document.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of cleaned MeSH terms.\n",
    "    \"\"\"\n",
    "    if isinstance(mesh_terms, str):\n",
    "        terms = mesh_terms.split(\";\")\n",
    "        return [re.sub(r\"/.*\", \"\", term).strip().lower() for term in terms]\n",
    "    return []\n",
    "\n",
    "# Step 3: Define the matching function using sensitive terms\n",
    "sensitive_pattern = re.compile(r\"\\b(\" + \"|\".join(re.escape(term) for term in sensitive_terms) + r\")\\b\", re.IGNORECASE)\n",
    "\n",
    "def is_sensitive_regex(terms):\n",
    "    \"\"\"\n",
    "    Check if any term in the document is sensitive based on MeSH terms.\n",
    "\n",
    "    Args:\n",
    "        terms (list of str): Processed MeSH terms.\n",
    "\n",
    "    Returns:\n",
    "        int: 1 if sensitive, 0 otherwise.\n",
    "    \"\"\"\n",
    "    return 1 if any(sensitive_pattern.search(term) for term in terms) else 0\n",
    "\n",
    "# Step 4: Apply preprocessing and matching to datasets\n",
    "for df in [training, validation, test]:\n",
    "    df.loc[:, \"processed_mesh_terms\"] = df[\"mesh_terms\"].apply(preprocess_mesh_terms)\n",
    "    df.loc[:, \"sensitive\"] = df[\"processed_mesh_terms\"].apply(is_sensitive_regex)\n",
    "    df.loc[:, \"label\"] = df[\"sensitive\"]\n",
    "\n",
    "# Step 5: Calculate sensitive document percentages\n",
    "datasets = [\n",
    "    (pd.concat([training, validation, test]), \"Full dataset\"),\n",
    "    (test, \"Judged documents\")\n",
    "]\n",
    "\n",
    "for df, name in datasets:\n",
    "    percentage = df[\"label\"].mean() * 100\n",
    "    print(f\"{name}: {percentage:.1f}%\")\n",
    "\n",
    "for df in [training, validation, test]:\n",
    "    df.drop(columns=[\"sensitive\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "840c6972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitive documents in Training:\n",
      "                                               mesh_terms  \\\n",
      "54170   Adult; Cardiovascular Diseases/ET/*MO; Diabete...   \n",
      "3827    Antigens, Bacterial/*AN; Chlamydia trachomatis...   \n",
      "63723   Adult; Evaluation Studies; Female; Hospitals, ...   \n",
      "144305  Adult; Antineoplastic Agents, Combined/*TU; Ca...   \n",
      "160573  Acidosis, Renal Tubular/*CO/ET; Adult; Case Re...   \n",
      "\n",
      "                                     processed_mesh_terms  label  \n",
      "54170   [adult, cardiovascular diseases, diabetes mell...      1  \n",
      "3827    [antigens, bacterial, chlamydia trachomatis, c...      1  \n",
      "63723   [adult, evaluation studies, female, hospitals,...      1  \n",
      "144305  [adult, antineoplastic agents, combined, case ...      1  \n",
      "160573  [acidosis, renal tubular, adult, case report, ...      1  \n",
      "Total sensitive documents in Training: 22371\n",
      "Relative amount of sensitive documents in Training: 7.9%\n",
      "\n",
      "Sensitive documents in Validation:\n",
      "                                               mesh_terms  \\\n",
      "129141  Electrocoagulation/AE/*IS/MT; Female; Follow-U...   \n",
      "170093  Adult; Child; Child, Preschool; Human; Hypospa...   \n",
      "174428  Dimercaptosuccinic Acid/*DU; Human; Organometa...   \n",
      "229509  Cohort Studies; Female; Human; Hypertension/CO...   \n",
      "202123  Adolescence; Adult; Catecholamines/*BL; Child;...   \n",
      "\n",
      "                                     processed_mesh_terms  label  \n",
      "129141  [electrocoagulation, female, follow-up studies...      1  \n",
      "170093  [adult, child, child, preschool, human, hyposp...      1  \n",
      "174428  [dimercaptosuccinic acid, human, organometalli...      1  \n",
      "229509  [cohort studies, female, human, hypertension, ...      1  \n",
      "202123  [adolescence, adult, catecholamines, child, ch...      1  \n",
      "Total sensitive documents in Validation: 4007\n",
      "Relative amount of sensitive documents in Validation: 8.0%\n",
      "\n",
      "Sensitive documents in Test:\n",
      "                                             mesh_terms  \\\n",
      "153   Female; Human; Pregnancy; Propranolol/TU; Puer...   \n",
      "222   Actinomycosis/*CO; Adult; Case Report; Human; ...   \n",
      "625   Adult; Analysis of Variance; Bromocriptine/AD/...   \n",
      "1362  Adult; Case Report; Disseminated Intravascular...   \n",
      "1504  Adult; Aged; Biopsy; Blood Platelets/*ME; Bloo...   \n",
      "\n",
      "                                   processed_mesh_terms  label  \n",
      "153   [female, human, pregnancy, propranolol, puerpe...      1  \n",
      "222   [actinomycosis, adult, case report, human, kid...      1  \n",
      "625   [adult, analysis of variance, bromocriptine, c...      1  \n",
      "1362  [adult, case report, disseminated intravascula...      1  \n",
      "1504  [adult, aged, biopsy, blood platelets, blood p...      1  \n",
      "Total sensitive documents in Test: 1781\n",
      "Relative amount of sensitive documents in Test: 12.3%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for df, name in [(training, \"Training\"), (validation, \"Validation\"), (test, \"Test\")]:\n",
    "    sensitive_docs = df[df[\"label\"] == 1]\n",
    "    total_docs = len(df)\n",
    "    relative_percentage = (len(sensitive_docs) / total_docs) * 100\n",
    "    \n",
    "    print(f\"Sensitive documents in {name}:\")\n",
    "    print(sensitive_docs[[\"mesh_terms\", \"processed_mesh_terms\", \"label\"]].head())\n",
    "    print(f\"Total sensitive documents in {name}: {len(sensitive_docs)}\")\n",
    "    print(f\"Relative amount of sensitive documents in {name}: {relative_percentage:.1f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60b3fe403a111283",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T00:23:25.188858Z",
     "start_time": "2025-01-03T00:23:18.525960Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "train_dataset = Dataset.from_pandas(training)\n",
    "test_dataset = Dataset.from_pandas(test)\n",
    "validation_dataset = Dataset.from_pandas(validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b333bd941a53a20b",
   "metadata": {},
   "source": [
    "Tokenizing sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2375df8226a5ec6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T01:02:21.211707Z",
     "start_time": "2025-01-03T00:23:25.250845Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a59d8348149742e18ee71ec18e42c4c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/284015 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize_function\u001b[39m(examples):\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer(examples[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle_abstract\u001b[39m\u001b[38;5;124m\"\u001b[39m], truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m train_dataset\u001b[38;5;241m.\u001b[39mmap(tokenize_function, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      9\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m test_dataset\u001b[38;5;241m.\u001b[39mmap(tokenize_function, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     10\u001b[0m validation_dataset \u001b[38;5;241m=\u001b[39m validation_dataset\u001b[38;5;241m.\u001b[39mmap(tokenize_function, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\datasets\\arrow_dataset.py:560\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    553\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[0;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[0;32m    558\u001b[0m }\n\u001b[0;32m    559\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[1;32m--> 560\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    561\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[0;32m    562\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\datasets\\arrow_dataset.py:3073\u001b[0m, in \u001b[0;36mDataset.map\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   3067\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3068\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[0;32m   3069\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3070\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[0;32m   3071\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   3072\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m-> 3073\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[0;32m   3074\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m   3075\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\datasets\\arrow_dataset.py:3476\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[1;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[0;32m   3472\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m   3473\u001b[0m     \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mslice\u001b[39m(i, i \u001b[38;5;241m+\u001b[39m batch_size)\u001b[38;5;241m.\u001b[39mindices(shard\u001b[38;5;241m.\u001b[39mnum_rows)))\n\u001b[0;32m   3474\u001b[0m )  \u001b[38;5;66;03m# Something simpler?\u001b[39;00m\n\u001b[0;32m   3475\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3476\u001b[0m     batch \u001b[38;5;241m=\u001b[39m apply_function_on_filtered_inputs(\n\u001b[0;32m   3477\u001b[0m         batch,\n\u001b[0;32m   3478\u001b[0m         indices,\n\u001b[0;32m   3479\u001b[0m         check_same_num_examples\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(shard\u001b[38;5;241m.\u001b[39mlist_indexes()) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   3480\u001b[0m         offset\u001b[38;5;241m=\u001b[39moffset,\n\u001b[0;32m   3481\u001b[0m     )\n\u001b[0;32m   3482\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[0;32m   3483\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[0;32m   3484\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3485\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\datasets\\arrow_dataset.py:3338\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[1;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[0;32m   3336\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[0;32m   3337\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[1;32m-> 3338\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m function(\u001b[38;5;241m*\u001b[39mfn_args, \u001b[38;5;241m*\u001b[39madditional_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_kwargs)\n\u001b[0;32m   3339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[0;32m   3340\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   3341\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[0;32m   3342\u001b[0m     }\n",
      "Cell \u001b[1;32mIn[13], line 6\u001b[0m, in \u001b[0;36mtokenize_function\u001b[1;34m(examples)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize_function\u001b[39m(examples):\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer(examples[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle_abstract\u001b[39m\u001b[38;5;124m\"\u001b[39m], truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\tokenization_utils_base.py:2860\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2858\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[0;32m   2859\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[1;32m-> 2860\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_one(text\u001b[38;5;241m=\u001b[39mtext, text_pair\u001b[38;5;241m=\u001b[39mtext_pair, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mall_kwargs)\n\u001b[0;32m   2861\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2862\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\tokenization_utils_base.py:2948\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[0;32m   2943\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2944\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match batch length of `text_pair`:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2945\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2946\u001b[0m         )\n\u001b[0;32m   2947\u001b[0m     batch_text_or_text_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[1;32m-> 2948\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_encode_plus(\n\u001b[0;32m   2949\u001b[0m         batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   2950\u001b[0m         add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   2951\u001b[0m         padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m   2952\u001b[0m         truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   2953\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m   2954\u001b[0m         stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[0;32m   2955\u001b[0m         is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[0;32m   2956\u001b[0m         pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   2957\u001b[0m         padding_side\u001b[38;5;241m=\u001b[39mpadding_side,\n\u001b[0;32m   2958\u001b[0m         return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[0;32m   2959\u001b[0m         return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   2960\u001b[0m         return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   2961\u001b[0m         return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   2962\u001b[0m         return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   2963\u001b[0m         return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   2964\u001b[0m         return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[0;32m   2965\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m   2966\u001b[0m         split_special_tokens\u001b[38;5;241m=\u001b[39msplit_special_tokens,\n\u001b[0;32m   2967\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2968\u001b[0m     )\n\u001b[0;32m   2969\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2970\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[0;32m   2971\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[0;32m   2972\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2990\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2991\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\tokenization_utils_base.py:3150\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[0;32m   3140\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m   3141\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   3142\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[0;32m   3143\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3147\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3148\u001b[0m )\n\u001b[1;32m-> 3150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_encode_plus(\n\u001b[0;32m   3151\u001b[0m     batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   3152\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[0;32m   3153\u001b[0m     padding_strategy\u001b[38;5;241m=\u001b[39mpadding_strategy,\n\u001b[0;32m   3154\u001b[0m     truncation_strategy\u001b[38;5;241m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   3155\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mmax_length,\n\u001b[0;32m   3156\u001b[0m     stride\u001b[38;5;241m=\u001b[39mstride,\n\u001b[0;32m   3157\u001b[0m     is_split_into_words\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[0;32m   3158\u001b[0m     pad_to_multiple_of\u001b[38;5;241m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   3159\u001b[0m     padding_side\u001b[38;5;241m=\u001b[39mpadding_side,\n\u001b[0;32m   3160\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39mreturn_tensors,\n\u001b[0;32m   3161\u001b[0m     return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   3162\u001b[0m     return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   3163\u001b[0m     return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   3164\u001b[0m     return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   3165\u001b[0m     return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   3166\u001b[0m     return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[0;32m   3167\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m   3168\u001b[0m     split_special_tokens\u001b[38;5;241m=\u001b[39msplit_special_tokens,\n\u001b[0;32m   3169\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3170\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\tokenization_utils.py:888\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[0;32m    885\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    886\u001b[0m     ids, pair_ids \u001b[38;5;241m=\u001b[39m ids_or_pair_ids\n\u001b[1;32m--> 888\u001b[0m first_ids \u001b[38;5;241m=\u001b[39m get_input_ids(ids)\n\u001b[0;32m    889\u001b[0m second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(pair_ids) \u001b[38;5;28;01mif\u001b[39;00m pair_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    890\u001b[0m input_ids\u001b[38;5;241m.\u001b[39mappend((first_ids, second_ids))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\tokenization_utils.py:855\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_encode_plus.<locals>.get_input_ids\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m    853\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_input_ids\u001b[39m(text):\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 855\u001b[0m         tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenize(text, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    856\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(tokens)\n\u001b[0;32m    857\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\tokenization_utils.py:654\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.tokenize\u001b[1;34m(self, text, **kwargs)\u001b[0m\n\u001b[0;32m    648\u001b[0m     escaped_special_toks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    649\u001b[0m         re\u001b[38;5;241m.\u001b[39mescape(s_tok\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[0;32m    650\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m s_tok \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_added_tokens_decoder\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m    651\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s_tok\u001b[38;5;241m.\u001b[39mspecial \u001b[38;5;129;01mand\u001b[39;00m s_tok\u001b[38;5;241m.\u001b[39mnormalized\n\u001b[0;32m    652\u001b[0m     ]\n\u001b[0;32m    653\u001b[0m     pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(escaped_special_toks) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)|\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(.+?)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 654\u001b[0m     text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(pattern, \u001b[38;5;28;01mlambda\u001b[39;00m m: m\u001b[38;5;241m.\u001b[39mgroups()[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m m\u001b[38;5;241m.\u001b[39mgroups()[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mlower(), text)\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m split_special_tokens:\n\u001b[0;32m    657\u001b[0m     no_split_token \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\timgr\\anaconda3\\Lib\\re\\__init__.py:186\u001b[0m, in \u001b[0;36msub\u001b[1;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msub\u001b[39m(pattern, repl, string, count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    180\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the string obtained by replacing the leftmost\u001b[39;00m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;124;03m    non-overlapping occurrences of the pattern in string by the\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m    replacement repl.  repl can be either a string or a callable;\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m    if a string, backslash escapes in it are processed.  If it is\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;124;03m    a callable, it's passed the Match object and must return\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;124;03m    a replacement string to be used.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _compile(pattern, flags)\u001b[38;5;241m.\u001b[39msub(repl, string, count)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\tokenization_utils.py:654\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.tokenize.<locals>.<lambda>\u001b[1;34m(m)\u001b[0m\n\u001b[0;32m    648\u001b[0m     escaped_special_toks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    649\u001b[0m         re\u001b[38;5;241m.\u001b[39mescape(s_tok\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[0;32m    650\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m s_tok \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_added_tokens_decoder\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m    651\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s_tok\u001b[38;5;241m.\u001b[39mspecial \u001b[38;5;129;01mand\u001b[39;00m s_tok\u001b[38;5;241m.\u001b[39mnormalized\n\u001b[0;32m    652\u001b[0m     ]\n\u001b[0;32m    653\u001b[0m     pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(escaped_special_toks) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)|\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(.+?)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 654\u001b[0m     text \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(pattern, \u001b[38;5;28;01mlambda\u001b[39;00m m: m\u001b[38;5;241m.\u001b[39mgroups()[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m m\u001b[38;5;241m.\u001b[39mgroups()[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mlower(), text)\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m split_special_tokens:\n\u001b[0;32m    657\u001b[0m     no_split_token \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Tokenize the dataset for BERTs training\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"title_abstract\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "validation_dataset = validation_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "train_dataset = train_dataset.remove_columns([\"title_abstract\"])\n",
    "test_dataset = test_dataset.remove_columns([\"title_abstract\"])\n",
    "validation_dataset = validation_dataset.remove_columns([\"title_abstract\"])\n",
    "\n",
    "train_dataset.set_format(\"torch\")\n",
    "test_dataset.set_format(\"torch\")\n",
    "validation_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32804bf6341b85c",
   "metadata": {},
   "source": [
    "Loading DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2832a16280a76b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T01:03:01.842828Z",
     "start_time": "2025-01-03T01:03:01.830931Z"
    }
   },
   "outputs": [],
   "source": [
    "print(training.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7ef9f777cd8f50",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-01-03T01:03:07.234154Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, fbeta_score\n",
    "\n",
    "# Define the compute_metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "\n",
    "    precision = precision_score(labels, predictions, average=\"binary\")\n",
    "    recall = recall_score(labels, predictions, average=\"binary\")\n",
    "    f1 = f1_score(labels, predictions, average=\"binary\")\n",
    "    f2 = fbeta_score(labels, predictions, beta=2, average=\"binary\")\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"f2\": f2,\n",
    "    }\n",
    "\n",
    "# Load the model with the number of labels\n",
    "num_labels = len(set(training[\"label\"]))\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=num_labels\n",
    ")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",                  # Directory to save models and checkpoints\n",
    "    evaluation_strategy=\"epoch\",            # Evaluate after each epoch\n",
    "    #learning_rate=2e-5,                     # Learning rate\n",
    "    #per_device_train_batch_size=16,         # Training batch size\n",
    "    #per_device_eval_batch_size=16,          # Evaluation batch size\n",
    "    #num_train_epochs=3,                     # Number of epochs\n",
    "    #weight_decay=0.01,                      # Regularization weight decay\n",
    "    logging_dir=\"./logs\",                   # Directory for logs\n",
    "    logging_steps=10,                       # Log every 10 steps\n",
    "    save_strategy=\"epoch\",                  # Save checkpoint after each epoch\n",
    "    metric_for_best_model=\"f1\",             # Choose F1 as the metric to optimize\n",
    "    #load_best_model_at_end=True             # Load the best model at the end of training\n",
    ")\n",
    "\n",
    "# Defining the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                            # Model to train\n",
    "    args=training_args,                     # Training arguments\n",
    "    train_dataset=train_dataset,            # Training dataset\n",
    "    eval_dataset=validation_dataset,        # Validation dataset\n",
    "    tokenizer=tokenizer,                    # Tokenizer for data processing\n",
    "    compute_metrics=compute_metrics         # Function to compute metrics\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "\n",
    "print(\"Test Metrics:\", test_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb77a2f6",
   "metadata": {},
   "source": [
    "## lets create a function where we try the different thresholds in the validation set and then "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892c48eb4e3d2c9e",
   "metadata": {},
   "source": [
    "Defining trainer and finetuning BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5274c60a10fbd90",
   "metadata": {},
   "source": [
    "Training and evaluating"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
