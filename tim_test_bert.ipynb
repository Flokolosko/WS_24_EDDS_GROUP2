{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Data:\n",
    "    - ohsumed 88 - 91 contains the normal ohsumed data \n",
    "    - qrels.ohsu has the relevance labels for the corresponding UIs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_relevance_file(filename):\n",
    "    relevance_dict = {}\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                query_id, medline_id, relevance = line.strip().split()\n",
    "                # Store by medline_id since we'll join on this\n",
    "                if medline_id not in relevance_dict:\n",
    "                    relevance_dict[medline_id] = []\n",
    "                relevance_dict[medline_id].append({\n",
    "                    'query_id': query_id,\n",
    "                    'relevance': int(relevance)\n",
    "                })\n",
    "            except ValueError:\n",
    "                continue  # Skip malformed lines\n",
    "    \n",
    "    print(f\"Total medline IDs with relevance judgments: {len(relevance_dict)}\")\n",
    "    return relevance_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available splits: dict_keys(['train', 'test'])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m     all_docs\u001b[38;5;241m.\u001b[39mextend(dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[1;32m---> 12\u001b[0m     all_docs\u001b[38;5;241m.\u001b[39mextend(dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal entries in combined dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(all_docs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\datasets\\arrow_dataset.py:2387\u001b[0m, in \u001b[0;36mDataset.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2385\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info\u001b[38;5;241m.\u001b[39mfeatures, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[0;32m   2386\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mARROW_READER_BATCH_SIZE_IN_DATASET_ITER\n\u001b[1;32m-> 2387\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m pa_subtable \u001b[38;5;129;01min\u001b[39;00m table_iter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata, batch_size\u001b[38;5;241m=\u001b[39mbatch_size):\n\u001b[0;32m   2388\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(pa_subtable\u001b[38;5;241m.\u001b[39mnum_rows):\n\u001b[0;32m   2389\u001b[0m         pa_subtable_ex \u001b[38;5;241m=\u001b[39m pa_subtable\u001b[38;5;241m.\u001b[39mslice(i, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\datasets\\table.py:2402\u001b[0m, in \u001b[0;36mtable_iter\u001b[1;34m(table, batch_size, drop_last_batch)\u001b[0m\n\u001b[0;32m   2400\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m chunks_buffer_size \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk) \u001b[38;5;241m==\u001b[39m batch_size:\n\u001b[0;32m   2401\u001b[0m     chunks_buffer\u001b[38;5;241m.\u001b[39mappend(chunk)\n\u001b[1;32m-> 2402\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mTable\u001b[38;5;241m.\u001b[39mfrom_batches(chunks_buffer)\n\u001b[0;32m   2403\u001b[0m     chunks_buffer \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   2404\u001b[0m     chunks_buffer_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load both train and test splits\n",
    "dataset = load_dataset(\"community-datasets/ohsumed\")\n",
    "print(\"Available splits:\", dataset.keys())\n",
    "\n",
    "# Combine train and test if both exist\n",
    "all_docs = []\n",
    "if 'train' in dataset:\n",
    "    all_docs.extend(dataset['train'])\n",
    "if 'test' in dataset:\n",
    "    all_docs.extend(dataset['test'])\n",
    "\n",
    "print(f\"Total entries in combined dataset: {len(all_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total medline IDs with relevance judgments: 3121\n",
      "Documents with relevance judgments: 3205\n",
      "Documents without relevance judgments: 345443\n"
     ]
    }
   ],
   "source": [
    "relevance_dict = parse_relevance_file('qrels.ohsu.88-91')\n",
    "\n",
    "# Combine with relevance information\n",
    "docs_with_relevance = []\n",
    "docs_without_relevance = []\n",
    "\n",
    "for doc in all_docs:\n",
    "    medline_id = str(doc['medline_ui'])\n",
    "    \n",
    "    if medline_id in relevance_dict:\n",
    "        for rel_info in relevance_dict[medline_id]:\n",
    "            doc_with_relevance = doc.copy()\n",
    "            doc_with_relevance['query_id'] = rel_info['query_id']\n",
    "            doc_with_relevance['relevance'] = rel_info['relevance']\n",
    "            docs_with_relevance.append(doc_with_relevance)\n",
    "    else:\n",
    "        docs_without_relevance.append(doc)\n",
    "\n",
    "print(f\"Documents with relevance judgments: {len(docs_with_relevance)}\")\n",
    "print(f\"Documents without relevance judgments: {len(docs_without_relevance)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved files:\n",
      "1. ohsumed_with_relevance.csv\n",
      "2. ohsumed_without_relevance.csv\n",
      "\n",
      "Columns in with_relevance dataset:\n",
      "['seq_id', 'medline_ui', 'mesh_terms', 'title', 'publication_type', 'abstract', 'author', 'source', 'query_id', 'relevance']\n",
      "\n",
      "Columns in without_relevance dataset:\n",
      "['seq_id', 'medline_ui', 'mesh_terms', 'title', 'publication_type', 'abstract', 'author', 'source']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert to DataFrames and save as CSVs\n",
    "# First the documents with relevance\n",
    "df_with_relevance = pd.DataFrame(docs_with_relevance)\n",
    "df_without_relevance = pd.DataFrame(docs_without_relevance)\n",
    "\n",
    "# Save with relevance dataset\n",
    "df_with_relevance.to_csv('ohsumed_with_relevance.csv', index=False)\n",
    "\n",
    "# Save without relevance dataset\n",
    "df_without_relevance.to_csv('ohsumed_without_relevance.csv', index=False)\n",
    "\n",
    "print(\"Saved files:\")\n",
    "print(\"1. ohsumed_with_relevance.csv\")\n",
    "print(\"2. ohsumed_without_relevance.csv\")\n",
    "\n",
    "# Print sample of columns to verify\n",
    "print(\"\\nColumns in with_relevance dataset:\")\n",
    "print(df_with_relevance.columns.tolist())\n",
    "print(\"\\nColumns in without_relevance dataset:\")\n",
    "print(df_without_relevance.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRepare data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes:\n",
      "Training set: 293626\n",
      "Validation set: 51817\n",
      "Test set: 3205\n",
      "\n",
      "Sensitivity distribution:\n",
      "Train set sensitive ratio: 0.07719343654853453\n",
      "Val set sensitive ratio: 0.07898952081363259\n",
      "Test set sensitive ratio: 0.09797191887675508\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First, let's add sensitivity labels to both datasets\n",
    "def is_sensitive(mesh_terms):\n",
    "    \"\"\"Check if document contains C12 or C13 related terms\"\"\"\n",
    "    sensitive_terms = ['urogenital', 'urinary', 'genital', 'reproductive', \n",
    "                      'pregnancy', 'gynecologic', 'obstetric', 'sexual', \n",
    "                      'fertility', 'prostate', 'testicular', 'ovarian', \n",
    "                      'uterine', 'vaginal', 'prenatal']\n",
    "    return 1 if any(term.lower() in str(mesh_terms).lower() for term in sensitive_terms) else 0\n",
    "\n",
    "# Load the CSVs\n",
    "docs_with_relevance = pd.read_csv('ohsumed_with_relevance.csv')\n",
    "docs_without_relevance = pd.read_csv('ohsumed_without_relevance.csv')\n",
    "\n",
    "# Add sensitivity labels\n",
    "docs_with_relevance['sensitive'] = docs_with_relevance['mesh_terms'].apply(is_sensitive)\n",
    "docs_without_relevance['sensitive'] = docs_without_relevance['mesh_terms'].apply(is_sensitive)\n",
    "\n",
    "# Following the paper's methodology:\n",
    "# 1. Test set = documents with relevance judgments\n",
    "test_set = docs_with_relevance\n",
    "\n",
    "# 2. Split remaining documents into train (85%) and validation (15%)\n",
    "train_set, val_set = train_test_split(docs_without_relevance, test_size=0.15, random_state=42)\n",
    "\n",
    "print(\"Dataset sizes:\")\n",
    "print(f\"Training set: {len(train_set)}\")\n",
    "print(f\"Validation set: {len(val_set)}\")\n",
    "print(f\"Test set: {len(test_set)}\")\n",
    "\n",
    "print(\"\\nSensitivity distribution:\")\n",
    "print(\"Train set sensitive ratio:\", train_set['sensitive'].mean())\n",
    "print(\"Val set sensitive ratio:\", val_set['sensitive'].mean())\n",
    "print(\"Test set sensitive ratio:\", test_set['sensitive'].mean())\n",
    "\n",
    "# Save the splits\n",
    "train_set.to_csv('ohsumed_train.csv', index=False)\n",
    "val_set.to_csv('ohsumed_val.csv', index=False)\n",
    "test_set.to_csv('ohsumed_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try Distilbert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 107\u001b[0m\n\u001b[0;32m    104\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m--> 107\u001b[0m     loss \u001b[38;5;241m=\u001b[39m train_epoch(model, train_loader, optimizer)\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[1;32mIn[39], line 66\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, train_loader, optimizer)\u001b[0m\n\u001b[0;32m     63\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(input_ids\u001b[38;5;241m=\u001b[39minput_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask, labels\u001b[38;5;241m=\u001b[39mlabels)\n\u001b[0;32m     64\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m---> 66\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     67\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     68\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    582\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    583\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[0;32m    348\u001b[0m     tensors,\n\u001b[0;32m    349\u001b[0m     grad_tensors_,\n\u001b[0;32m    350\u001b[0m     retain_graph,\n\u001b[0;32m    351\u001b[0m     create_graph,\n\u001b[0;32m    352\u001b[0m     inputs,\n\u001b[0;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    355\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, fbeta_score\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# Load our split datasets\n",
    "train_docs = pd.read_csv('ohsumed_train.csv')\n",
    "val_docs = pd.read_csv('ohsumed_val.csv')\n",
    "test_docs = pd.read_csv('ohsumed_test.csv')\n",
    "\n",
    "class OHSUMEDDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=max_length)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Fixed prepare_data function\n",
    "def prepare_data(docs):\n",
    "    # Using pandas DataFrame methods correctly\n",
    "    texts = [f\"{str(title)} {str(abstract)}\" for title, abstract in zip(docs['title'], docs['abstract'])]\n",
    "    labels = docs['sensitive'].tolist()\n",
    "    return texts, labels\n",
    "\n",
    "# Initialize model and tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Prepare datasets\n",
    "train_texts, train_labels = prepare_data(train_docs)\n",
    "val_texts, val_labels = prepare_data(val_docs)\n",
    "test_texts, test_labels = prepare_data(test_docs)\n",
    "\n",
    "train_dataset = OHSUMEDDataset(train_texts, train_labels, tokenizer)\n",
    "val_dataset = OHSUMEDDataset(val_texts, val_labels, tokenizer)\n",
    "test_dataset = OHSUMEDDataset(test_texts, test_labels, tokenizer)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# Training function\n",
    "def train_epoch(model, train_loader, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            \n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='binary')\n",
    "    f2 = fbeta_score(true_labels, predictions, beta=2)\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    \n",
    "    return {\n",
    "        'precision': precision * 100,\n",
    "        'recall': recall * 100,\n",
    "        'f1': f1 * 100,\n",
    "        'f2': f2 * 100,\n",
    "        'accuracy': accuracy * 100\n",
    "    }\n",
    "\n",
    "# Training loop\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss = train_epoch(model, train_loader, optimizer)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss:.4f}\")\n",
    "    \n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        metrics = evaluate(model, val_loader)\n",
    "        print(f\"Validation metrics:\", metrics)\n",
    "\n",
    "# Final evaluation on test set\n",
    "test_metrics = evaluate(model, test_loader)\n",
    "print(\"\\nTest metrics:\", test_metrics)\n",
    "\n",
    "# Compare with paper results\n",
    "paper_results = {\n",
    "    'precision': 82.75,\n",
    "    'recall': 80.08,\n",
    "    'f1': 81.39,\n",
    "    'f2': 80.60,\n",
    "    'accuracy': 95.52\n",
    "}\n",
    "\n",
    "print(\"\\nPaper results:\", paper_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
