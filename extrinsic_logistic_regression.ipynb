{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extrinsic Evaluation\n",
    "Post - Filter approach:\n",
    "\n",
    "- we use the sensitivity labels from our intrinsic evaluation \n",
    "  \n",
    "- in the post filter approach we rank the documents according to the  coordinate ascent algorithm optimizing towards normalized\n",
    "Discounted Cumulative Gain (nDCG)\n",
    "\n",
    "- for that we use predefined functions from https://github.com/rueycheng/CoordinateAscent/blob/master as the implementation was not mentioned in the paper\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predicted = pd.read_csv(\"intermediate_results/logistic_regression_test_predicted.csv\")\n",
    "\n",
    "query_groups = test_predicted.groupby(\"Query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predicted[\"Query\"] = test_predicted[\"Query\"].astype(int)\n",
    "test_predicted[\"title_abstract\"] = test_predicted[\"title_abstract\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>query_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>.B 60 year old menopausal woman without hormon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>.B 60 yo male with disseminated intravascular ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>.B prolonged prothrombin time anticardiolipin ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>.B 88 yo with subdural reviews on subdurals in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>.B 58 yo with cancer and hypercalcemia effecti...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   query_id                                         query_text\n",
       "0         1  .B 60 year old menopausal woman without hormon...\n",
       "1         2  .B 60 yo male with disseminated intravascular ...\n",
       "2         3  .B prolonged prothrombin time anticardiolipin ...\n",
       "3         4  .B 88 yo with subdural reviews on subdurals in...\n",
       "4         5  .B 58 yo with cancer and hypercalcemia effecti..."
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_queries_alternate(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    queries = []\n",
    "    current_id = None\n",
    "    current_text = []\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line.startswith(\".I\"):\n",
    "            if current_id is not None:\n",
    "                queries.append({\"query_id\": current_id, \"query_text\": \" \".join(current_text)})\n",
    "            current_id = int(line.split()[1])\n",
    "            current_text = []\n",
    "        elif line.startswith(\".W\"):\n",
    "            continue  # Skip the .W line\n",
    "        else:\n",
    "            current_text.append(line)\n",
    "    \n",
    "    # Add the last query\n",
    "    if current_id is not None:\n",
    "        queries.append({\"query_id\": current_id, \"query_text\": \" \".join(current_text)})\n",
    "    \n",
    "    return pd.DataFrame(queries)\n",
    "\n",
    "# Parse the files using the alternate function\n",
    "queries1 = parse_queries_alternate(\"data/Queries1.txt\")\n",
    "queries2 = parse_queries_alternate(\"data/Queries2.txt\")\n",
    "\n",
    "# Combine the queries\n",
    "queries_df = pd.concat([queries1, queries2], ignore_index=True)\n",
    "\n",
    "# Verify\n",
    "queries_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predicted = test_predicted.merge(queries_df, left_on=\"Query\", right_on=\"query_id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we are going to use the TFIDF vectorization to compare the queries with our title and abstract and use this similiarity for the ranking algorithm\n",
    "-> problem is they never mentioned in the paper what they used to compute these scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "corpus = test_predicted[\"query_text\"].tolist() + test_predicted[\"title_abstract\"].tolist()\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "n_queries = len(test_predicted[\"query_text\"])\n",
    "query_tfidf = tfidf_matrix[:n_queries]\n",
    "doc_tfidf = tfidf_matrix[n_queries:]\n",
    "\n",
    "similarities = cosine_similarity(query_tfidf, doc_tfidf)\n",
    "\n",
    "test_predicted[\"tfidf_similarity\"] = similarities.diagonal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "values need to be between 0 and 2 \n",
    "(as we are looking at the ration this wont make a difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Rescale TF-IDF similarities to the range [0, 2]\n",
    "scaler = MinMaxScaler(feature_range=(0, 2))\n",
    "test_predicted[\"tfidf_similarity\"] = scaler.fit_transform(\n",
    "    test_predicted[[\"tfidf_similarity\"]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    28860.000000\n",
      "mean         0.284187\n",
      "std          0.246498\n",
      "min          0.000000\n",
      "25%          0.097436\n",
      "50%          0.231182\n",
      "75%          0.407963\n",
      "max          2.000000\n",
      "Name: tfidf_similarity, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(test_predicted[\"tfidf_similarity\"].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_predicted[\"tfidf_similarity\"].values\n",
    "y_test = test_predicted[\"Relevance_total\"].values\n",
    "qid_test= test_predicted[\"Query\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we use predefined coordinate ascent and ndcgscorer modules here. They can be found on: https://github.com/rueycheng/CoordinateAscent/blob/master   \n",
    "\n",
    "- we use each queryid and group the according entries to the queries and calculate the average ndcg_score for the entire test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.coordinate_ascent import CoordinateAscent\n",
    "from utils.metrics import NDCGScorer\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "scorer = NDCGScorer(k=10, idcg_cache={})\n",
    "\n",
    "# Convert X_test to a sparse matrix\n",
    "X_test_sparse = csr_matrix(X_test)\n",
    "\n",
    "model = CoordinateAscent(n_restarts=2, max_iter=25, verbose=True, scorer=scorer).fit(X_test_sparse, y_test, qid_test)\n",
    "\n",
    "pred = model.predict(X_test_sparse, qid_test)\n",
    "\n",
    "test_predicted[\"predicted_scores\"] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predicted_filtered = test_predicted[test_predicted[\"predicted_label\"] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_filtered = test_predicted_filtered[\"tfidf_similarity\"].values.reshape(-1, 1)  # Feature matrix\n",
    "y_test_filtered = test_predicted_filtered[\"Relevance_total\"].values  # Relevance labels\n",
    "qid_test_filtered = test_predicted_filtered[\"Query\"].values  # Query IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Postfilter Average nCS-DCG@10: 0.3017\n"
     ]
    }
   ],
   "source": [
    "ndcg_score = scorer(y_test_filtered, test_predicted_filtered[\"predicted_scores\"].values, qid_test_filtered).mean()\n",
    "\n",
    "# Print the final score\n",
    "print(f\"Postfilter Average nCS-DCG@10: {ndcg_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as these results are not very good, we are going to try the approach that is mentioned in paper 28 that they mentioned\n",
    "\n",
    "in paper 28 they mentioned some other feature meassures such as BM250 and the proximity_count so we are going to try this aswell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this runs for a very long time so its commented out\n",
    "\n",
    "#from rank_bm25 import BM25Okapi\n",
    "#\n",
    "## Tokenize documents and queries\n",
    "#documents = [doc.split() for doc in test_predicted[\"title_abstract\"]]\n",
    "#queries = [query.split() for query in test_predicted[\"query_text\"]]\n",
    "#\n",
    "## Initialize BM25 model\n",
    "#bm25 = BM25Okapi(documents)\n",
    "#\n",
    "## Compute BM25 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predicted = pd.read_csv(\"intermediate_results/test_predicted_withBM25.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we also use proxmity count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proximity_count(query, doc, window=8):\n",
    "    \"\"\"\n",
    "    Counts the number of windows within the document that contain at least one query term.\n",
    "    Optionally, weighs matches based on the number of query terms in the window.\n",
    "    \"\"\"\n",
    "    # Tokenize the query and document\n",
    "    query_terms = query.split()\n",
    "    doc_terms = doc.split()\n",
    "    \n",
    "    count = 0\n",
    "    \n",
    "    # Slide a window over the document terms\n",
    "    for i in range(len(doc_terms) - window + 1):\n",
    "        window_terms = doc_terms[i:i + window]\n",
    "        overlap = set(query_terms) & set(window_terms)  # Intersection of query terms and window terms\n",
    "        count += len(overlap)  # Optionally, count unique query terms in the window\n",
    "    \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    28860.000000\n",
      "mean        79.681635\n",
      "std         69.239140\n",
      "min          0.000000\n",
      "25%         24.000000\n",
      "50%         68.000000\n",
      "75%        120.000000\n",
      "max        591.000000\n",
      "Name: proximity_count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "test_predicted[\"proximity_count\"] = test_predicted.apply(\n",
    "    lambda row: proximity_count(row[\"query_text\"], row[\"title_abstract\"]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Inspect proximity counts\n",
    "print(test_predicted[\"proximity_count\"].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_predicted[[\"bm25_score\", \"proximity_count\"]].values\n",
    "y_test = test_predicted[\"Relevance_total\"].values\n",
    "qid_test= test_predicted[\"Query\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict the actual scores with our scorer and our CoordinateAscent algorithm using the bm25_score and the proximity_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t1\t1\t0.30425724608270827\n",
      "1\t2\t0\t0.30458623209584335\n",
      "1\t1\t1\t0.30490724996267293\n",
      "1\t2\t0\t0.30493944056703975\n",
      "1\t1\t0\t0.3049623310198546\n",
      "1\t2\t1\t0.3049945216242214\n",
      "2\t1\t0\t0.30458623209584335\n",
      "2\t2\t1\t0.30494094090988433\n",
      "2\t1\t1\t0.30495510080330307\n",
      "2\t2\t0\t0.3049623310198546\n",
      "2\t1\t0\t0.3049945216242214\n"
     ]
    }
   ],
   "source": [
    "scorer = NDCGScorer(k=10, idcg_cache={})\n",
    "\n",
    "X_test_sparse = csr_matrix(X_test)\n",
    "\n",
    "model = CoordinateAscent(n_restarts=2, max_iter=25, verbose=True, scorer=scorer).fit(X_test_sparse, y_test, qid_test)\n",
    "\n",
    "pred = model.predict(X_test_sparse, qid_test)\n",
    "\n",
    "test_predicted[\"predicted_scores\"] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predicted_filtered= test_predicted[test_predicted[\"predicted_label\"] == 0]\n",
    "\n",
    "X_test_filtered = test_predicted_filtered[[\"bm25_score\", \"proximity_count\"]].values\n",
    "y_test_filtered = test_predicted_filtered[\"Relevance_total\"].values  # Relevance labels\n",
    "qid_test_filtered = test_predicted_filtered[\"Query\"].values  # Query IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Postfilter Average nCS-DCG@10: 0.3016\n"
     ]
    }
   ],
   "source": [
    "ndcg_score = scorer(y_test_filtered, test_predicted_filtered[\"predicted_scores\"].values, qid_test_filtered).mean()\n",
    "\n",
    "# Print the final score\n",
    "print(f\"Postfilter Average nCS-DCG@10: {ndcg_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joint - approach\n",
    "- here we want to find a balanced result between sensitivity and relevance \n",
    "- for that we apply the penalty for sensitvity directly during the ranking process\n",
    "- we again use the features from above, but also take sensitivity directly into account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [\"bm25_score\", \"proximity_count\"]\n",
    "X_joint = test_predicted[feature_columns].values\n",
    "\n",
    "y_joint = test_predicted[[\"Relevance_total\", \"predicted_label\"]].values\n",
    "qid_joint = test_predicted[\"Query\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_joint = csr_matrix(X_joint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in the paper it was mentioned that they applied a penalty of 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.metrics import NDCGScorer\n",
    "\n",
    "class nCS_DCGScorer:\n",
    "    def __init__(self, y_sensitivity, k=10, sensitivity_penalty=12, idcg_cache={}):\n",
    "        self.y_sensitivity = y_sensitivity  # Store sensitivity labels\n",
    "        self.k = k\n",
    "        self.sensitivity_penalty = sensitivity_penalty\n",
    "        self.ndcg_scorer = NDCGScorer(k=k, idcg_cache=idcg_cache)\n",
    "\n",
    "    def __call__(self, y_relevance, pred, qid):\n",
    "        # Apply sensitivity penalty to predicted scores\n",
    "        penalized_pred = pred.copy()\n",
    "        penalized_pred[self.y_sensitivity == 1] -= self.sensitivity_penalty\n",
    "\n",
    "        # Compute nDCG@10 with penalized predictions\n",
    "        return self.ndcg_scorer(y_relevance, penalized_pred, qid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t1\t1\t0.3042431191865809\n",
      "1\t2\t0\t0.30461842270021017\n",
      "1\t1\t0\t0.3046991298652665\n",
      "1\t2\t1\t0.305047633729398\n",
      "2\t1\t1\t0.3042431191865809\n",
      "2\t2\t0\t0.30461842270021017\n",
      "2\t1\t1\t0.30465121320050803\n",
      "2\t2\t0\t0.3046991298652665\n",
      "2\t1\t1\t0.305047633729398\n",
      "3\t1\t0\t0.30461842270021017\n",
      "3\t2\t1\t0.30465121320050803\n",
      "3\t1\t0\t0.3046991298652665\n",
      "3\t2\t1\t0.3049925526722163\n",
      "3\t1\t0\t0.3049952052072145\n",
      "3\t2\t1\t0.305047633729398\n",
      "4\t1\t0\t0.30461842270021017\n",
      "4\t2\t1\t0.30465121320050803\n",
      "4\t1\t1\t0.30502093433537214\n",
      "4\t2\t0\t0.3050296030184499\n",
      "4\t1\t1\t0.305047633729398\n",
      "5\t1\t1\t0.3042431191865809\n",
      "5\t2\t0\t0.30461842270021017\n",
      "5\t1\t0\t0.3046991298652665\n",
      "5\t2\t1\t0.305047633729398\n"
     ]
    }
   ],
   "source": [
    "y_relevance = y_joint[:, 0]\n",
    "y_sensitivity = y_joint[:, 1]\n",
    "\n",
    "ncs_dcg_scorer = nCS_DCGScorer(y_sensitivity=y_sensitivity, k=10, sensitivity_penalty=12)\n",
    "\n",
    "model = CoordinateAscent(\n",
    "    n_restarts=5,\n",
    "    max_iter=50,\n",
    "    verbose=True,\n",
    "    scorer=ncs_dcg_scorer  \n",
    ").fit(X_joint, y_relevance, qid_joint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joint Approach - Average nCS-DCG@10: 0.3050\n"
     ]
    }
   ],
   "source": [
    "pred_joint = model.predict(X_joint, qid_joint)\n",
    "\n",
    "test_predicted[\"joint_scores\"] = pred_joint\n",
    "\n",
    "average_ncs_dcg = ncs_dcg_scorer(y_relevance, pred_joint, qid_joint).mean()\n",
    "print(f\"Joint Approach - Average nCS-DCG@10: {average_ncs_dcg:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this does not seem reproducable either\n",
    "\n",
    "- the correct features they used were never mentioned, we can not tell how they experimental setup looked like here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
