{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35dc65b72dbf70a2",
   "metadata": {},
   "source": [
    "# Intrinsic Evaluation\n",
    "Firstly, we will load the *OHSUMED* data from different files and merge them into one combined dataframe, containing all years from 1987 - 1991. Afterwards, we will load the file where the relevance labels of documents during all five years will be labeled. In the end, we will left join the relevance labels to our main dataframe.\n",
    "\n",
    "This data was not given by the paper and we had to search for it ourselves which was quite challenging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e74b25fa83a4bba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T00:23:15.994221Z",
     "start_time": "2025-01-03T00:23:06.733592Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DistilBertTokenizer\n",
    "from transformers import DistilBertForSequenceClassification\n",
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def parse_ohsumed_file(file_path):\n",
    "    \"\"\"Parses an OHSUMED file into a DataFrame with proper column names.\"\"\"\n",
    "    documents = []\n",
    "    document = {}\n",
    "\n",
    "    # Read the file line by line\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()  # Remove extra whitespace\n",
    "\n",
    "            if line.startswith(\".I\"):  # New document identifier\n",
    "                if document:  # If there's an existing document, save it\n",
    "                    documents.append(document)\n",
    "                document = {\"sequential identifier\": line[3:]}  # Initialize a new document\n",
    "\n",
    "            elif line.startswith(\".U\"):  # MEDLINE identifier\n",
    "                document[\"MEDLINE identifier\"] = next(f).strip()\n",
    "\n",
    "            elif line.startswith(\".S\"):  # Source\n",
    "                document[\"source\"] = next(f).strip()\n",
    "\n",
    "            elif line.startswith(\".M\"):  # MeSH terms\n",
    "                document[\"mesh_terms\"] = next(f).strip()\n",
    "\n",
    "            elif line.startswith(\".T\"):  # Title\n",
    "                document[\"title\"] = next(f).strip()\n",
    "\n",
    "            elif line.startswith(\".P\"):  # Publication type\n",
    "                document[\"publication type\"] = next(f).strip()\n",
    "\n",
    "            elif line.startswith(\".W\"):  # Abstract\n",
    "                document[\"abstract\"] = next(f).strip()\n",
    "\n",
    "            elif line.startswith(\".A\"):  # Author\n",
    "                document[\"author\"] = next(f).strip()\n",
    "\n",
    "    # Add the last document if it exists\n",
    "    if document:\n",
    "        documents.append(document)\n",
    "\n",
    "    # Convert the list of documents into a DataFrame\n",
    "    return pd.DataFrame(documents)\n",
    "\n",
    "# File paths for individual files\n",
    "file_87_path = \"./data/ohsumed.87.txt\"\n",
    "file_88_path = \"./data/ohsumed.88.txt\"\n",
    "file_89_path = \"./data/ohsumed.89.txt\"\n",
    "file_90_path = \"./data/ohsumed.90.txt\"\n",
    "file_91_path = \"./data/ohsumed.91.txt\"\n",
    "\n",
    "# Parse each file into its own DataFrame\n",
    "df_ohsumed_87 = parse_ohsumed_file(file_87_path)\n",
    "df_ohsumed_88 = parse_ohsumed_file(file_88_path)\n",
    "df_ohsumed_89 = parse_ohsumed_file(file_89_path)\n",
    "df_ohsumed_90 = parse_ohsumed_file(file_90_path)\n",
    "df_ohsumed_91 = parse_ohsumed_file(file_91_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082f5598",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f82e80aca558d1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T00:23:16.293057Z",
     "start_time": "2025-01-03T00:23:16.282917Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in ohsumed.87: 54710\n",
      "Number of rows in ohsumed.88: 70825\n",
      "Number of rows in ohsumed.89: 74869\n",
      "Number of rows in ohsumed.90: 73824\n",
      "Number of rows in ohsumed.91: 74338\n"
     ]
    }
   ],
   "source": [
    "# Checking row amount of all years files\n",
    "print(f\"Number of rows in ohsumed.87: {len(df_ohsumed_87)}\")\n",
    "print(f\"Number of rows in ohsumed.88: {len(df_ohsumed_88)}\")\n",
    "print(f\"Number of rows in ohsumed.89: {len(df_ohsumed_89)}\")\n",
    "print(f\"Number of rows in ohsumed.90: {len(df_ohsumed_90)}\")\n",
    "print(f\"Number of rows in ohsumed.91: {len(df_ohsumed_91)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e40060215a7b92d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T00:23:16.358974Z",
     "start_time": "2025-01-03T00:23:16.303019Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined DataFrame shape: (348566, 8)\n",
      "  sequential identifier MEDLINE identifier                             source  \\\n",
      "0                     1           87049087    Am J Emerg Med 8703; 4(6):491-5   \n",
      "1                     2           87049088  Am J Emerg Med 8703; 4(6):496-500   \n",
      "2                     3           87049089    Am J Emerg Med 8703; 4(6):501-3   \n",
      "3                     4           87049090    Am J Emerg Med 8703; 4(6):504-6   \n",
      "4                     5           87049092    Am J Emerg Med 8703; 4(6):511-3   \n",
      "\n",
      "                                          mesh_terms  \\\n",
      "0  Allied Health Personnel/*; Electric Countersho...   \n",
      "1  Antidepressive Agents, Tricyclic/*PO; Arrhythm...   \n",
      "2  Adult; Aircraft/*; Altitude/*; Blood Gas Monit...   \n",
      "3  Adolescence; Adult; Aged; Blood Glucose/*ME; D...   \n",
      "4  Aged; Aged, 80 and over; Case Report; Female; ...   \n",
      "\n",
      "                                               title  publication type  \\\n",
      "0  Refibrillation managed by EMT-Ds: incidence an...  JOURNAL ARTICLE.   \n",
      "1  Tricyclic antidepressant overdose: emergency d...  JOURNAL ARTICLE.   \n",
      "2  Transconjunctival oxygen monitoring as a predi...  JOURNAL ARTICLE.   \n",
      "3  Serum glucose changes after administration of ...  JOURNAL ARTICLE.   \n",
      "4  Nasogastric intubation: morbidity in an asympt...  JOURNAL ARTICLE.   \n",
      "\n",
      "                                            abstract  \\\n",
      "0  Some patients converted from ventricular fibri...   \n",
      "1  There is controversy regarding the appropriate...   \n",
      "2  As the use of helicopters for air transport of...   \n",
      "3  A prospective clinical trial was conducted to ...   \n",
      "4  An unusual case of a misdirected nasogastric t...   \n",
      "\n",
      "                                              author  \n",
      "0                               Stults KR; Brown DD.  \n",
      "1                 Foulke GE; Albertson TE; Walby WF.  \n",
      "2  Shufflebarger C; Jehle D; Cottington E; Martin M.  \n",
      "3                                          Adler PM.  \n",
      "4                                   Gough D; Rust D.  \n"
     ]
    }
   ],
   "source": [
    "# Combine all DataFrames into a single DataFrame\n",
    "ohsumed_combined_df = pd.concat([df_ohsumed_87, df_ohsumed_88, df_ohsumed_89, df_ohsumed_90, df_ohsumed_91], ignore_index=True)\n",
    "\n",
    "# Print the combined DataFrame's shape (rows and columns). Should be: 348566\n",
    "print(f\"Combined DataFrame shape: {ohsumed_combined_df.shape}\")\n",
    "\n",
    "# Display the first few rows of the combined DataFrame\n",
    "print(ohsumed_combined_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdf1e75",
   "metadata": {},
   "source": [
    "to get the right relevance labels, we had to search for the \"judged.txt\". This was also quite hard and required an intensive online-search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1caf5d95e1d30c27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T00:23:16.421630Z",
     "start_time": "2025-01-03T00:23:16.382103Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded judged file with 16140 rows\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Document-UI</th>\n",
       "      <th>Document-Index</th>\n",
       "      <th>Relevance1</th>\n",
       "      <th>Relevance2</th>\n",
       "      <th>Relevance3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>87097544</td>\n",
       "      <td>40626</td>\n",
       "      <td>d</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>87153566</td>\n",
       "      <td>11852</td>\n",
       "      <td>n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>87157536</td>\n",
       "      <td>12693</td>\n",
       "      <td>d</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>87157537</td>\n",
       "      <td>12694</td>\n",
       "      <td>d</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>87184723</td>\n",
       "      <td>15450</td>\n",
       "      <td>n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Query  Document-UI  Document-Index Relevance1 Relevance2 Relevance3\n",
       "0      1     87097544           40626          d        NaN          d\n",
       "1      1     87153566           11852          n        NaN          n\n",
       "2      1     87157536           12693          d        NaN        NaN\n",
       "3      1     87157537           12694          d        NaN        NaN\n",
       "4      1     87184723           15450          n        NaN        NaN"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the judged file (relevance labeles)\n",
    "judged_df = pd.read_csv(\"./data/judged.txt\", sep=\"\\t\", header=None,\n",
    "                        names=[\"Query\", \"Document-UI\", \"Document-Index\", \"Relevance1\", \"Relevance2\", \"Relevance3\"])\n",
    "\n",
    "print(f\"Loaded judged file with {len(judged_df)} rows\")\n",
    "judged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "716274c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique documents: 14430\n",
      "New df size: 14430\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Document-UI</th>\n",
       "      <th>Document-Index</th>\n",
       "      <th>Relevance1</th>\n",
       "      <th>Relevance2</th>\n",
       "      <th>Relevance3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>87097544</td>\n",
       "      <td>40626</td>\n",
       "      <td>d</td>\n",
       "      <td>NaN</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>87153566</td>\n",
       "      <td>11852</td>\n",
       "      <td>n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>87157536</td>\n",
       "      <td>12693</td>\n",
       "      <td>d</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>87157537</td>\n",
       "      <td>12694</td>\n",
       "      <td>d</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>87184723</td>\n",
       "      <td>15450</td>\n",
       "      <td>n</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Query  Document-UI  Document-Index Relevance1 Relevance2 Relevance3\n",
       "0      1     87097544           40626          d        NaN          d\n",
       "1      1     87153566           11852          n        NaN          n\n",
       "2      1     87157536           12693          d        NaN        NaN\n",
       "3      1     87157537           12694          d        NaN        NaN\n",
       "4      1     87184723           15450          n        NaN        NaN"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get unique documents\n",
    "unique_docs = judged_df['Document-UI'].nunique()\n",
    "judged_df = judged_df.drop_duplicates(subset=['Document-UI'], keep='first')\n",
    "\n",
    "print(f\"Unique documents: {unique_docs}\")\n",
    "print(f\"New df size: {len(judged_df)}\")\n",
    "\n",
    "judged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fd89d53f138fcfc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T00:23:17.082326Z",
     "start_time": "2025-01-03T00:23:16.492793Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resulting DataFrame shape: (348566, 14)\n",
      "  sequential identifier  MEDLINE identifier  \\\n",
      "0                     1            87049087   \n",
      "1                     2            87049088   \n",
      "2                     3            87049089   \n",
      "3                     4            87049090   \n",
      "4                     5            87049092   \n",
      "\n",
      "                              source  \\\n",
      "0    Am J Emerg Med 8703; 4(6):491-5   \n",
      "1  Am J Emerg Med 8703; 4(6):496-500   \n",
      "2    Am J Emerg Med 8703; 4(6):501-3   \n",
      "3    Am J Emerg Med 8703; 4(6):504-6   \n",
      "4    Am J Emerg Med 8703; 4(6):511-3   \n",
      "\n",
      "                                          mesh_terms  \\\n",
      "0  Allied Health Personnel/*; Electric Countersho...   \n",
      "1  Antidepressive Agents, Tricyclic/*PO; Arrhythm...   \n",
      "2  Adult; Aircraft/*; Altitude/*; Blood Gas Monit...   \n",
      "3  Adolescence; Adult; Aged; Blood Glucose/*ME; D...   \n",
      "4  Aged; Aged, 80 and over; Case Report; Female; ...   \n",
      "\n",
      "                                               title  publication type  \\\n",
      "0  Refibrillation managed by EMT-Ds: incidence an...  JOURNAL ARTICLE.   \n",
      "1  Tricyclic antidepressant overdose: emergency d...  JOURNAL ARTICLE.   \n",
      "2  Transconjunctival oxygen monitoring as a predi...  JOURNAL ARTICLE.   \n",
      "3  Serum glucose changes after administration of ...  JOURNAL ARTICLE.   \n",
      "4  Nasogastric intubation: morbidity in an asympt...  JOURNAL ARTICLE.   \n",
      "\n",
      "                                            abstract  \\\n",
      "0  Some patients converted from ventricular fibri...   \n",
      "1  There is controversy regarding the appropriate...   \n",
      "2  As the use of helicopters for air transport of...   \n",
      "3  A prospective clinical trial was conducted to ...   \n",
      "4  An unusual case of a misdirected nasogastric t...   \n",
      "\n",
      "                                              author  Query  Document-Index  \\\n",
      "0                               Stults KR; Brown DD.    NaN             NaN   \n",
      "1                 Foulke GE; Albertson TE; Walby WF.    NaN             NaN   \n",
      "2  Shufflebarger C; Jehle D; Cottington E; Martin M.    NaN             NaN   \n",
      "3                                          Adler PM.    NaN             NaN   \n",
      "4                                   Gough D; Rust D.    NaN             NaN   \n",
      "\n",
      "  Relevance1 Relevance2 Relevance3  is_relevant_ind  \n",
      "0        NaN        NaN        NaN              NaN  \n",
      "1        NaN        NaN        NaN              NaN  \n",
      "2        NaN        NaN        NaN              NaN  \n",
      "3        NaN        NaN        NaN              NaN  \n",
      "4        NaN        NaN        NaN              NaN  \n",
      "Resulting DataFrame shape: (348566, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequential identifier</th>\n",
       "      <th>MEDLINE identifier</th>\n",
       "      <th>source</th>\n",
       "      <th>mesh_terms</th>\n",
       "      <th>title</th>\n",
       "      <th>publication type</th>\n",
       "      <th>abstract</th>\n",
       "      <th>author</th>\n",
       "      <th>is_relevant_ind</th>\n",
       "      <th>Relevance_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>87049087</td>\n",
       "      <td>Am J Emerg Med 8703; 4(6):491-5</td>\n",
       "      <td>Allied Health Personnel/*; Electric Countersho...</td>\n",
       "      <td>Refibrillation managed by EMT-Ds: incidence an...</td>\n",
       "      <td>JOURNAL ARTICLE.</td>\n",
       "      <td>Some patients converted from ventricular fibri...</td>\n",
       "      <td>Stults KR; Brown DD.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>87049088</td>\n",
       "      <td>Am J Emerg Med 8703; 4(6):496-500</td>\n",
       "      <td>Antidepressive Agents, Tricyclic/*PO; Arrhythm...</td>\n",
       "      <td>Tricyclic antidepressant overdose: emergency d...</td>\n",
       "      <td>JOURNAL ARTICLE.</td>\n",
       "      <td>There is controversy regarding the appropriate...</td>\n",
       "      <td>Foulke GE; Albertson TE; Walby WF.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>87049089</td>\n",
       "      <td>Am J Emerg Med 8703; 4(6):501-3</td>\n",
       "      <td>Adult; Aircraft/*; Altitude/*; Blood Gas Monit...</td>\n",
       "      <td>Transconjunctival oxygen monitoring as a predi...</td>\n",
       "      <td>JOURNAL ARTICLE.</td>\n",
       "      <td>As the use of helicopters for air transport of...</td>\n",
       "      <td>Shufflebarger C; Jehle D; Cottington E; Martin M.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>87049090</td>\n",
       "      <td>Am J Emerg Med 8703; 4(6):504-6</td>\n",
       "      <td>Adolescence; Adult; Aged; Blood Glucose/*ME; D...</td>\n",
       "      <td>Serum glucose changes after administration of ...</td>\n",
       "      <td>JOURNAL ARTICLE.</td>\n",
       "      <td>A prospective clinical trial was conducted to ...</td>\n",
       "      <td>Adler PM.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>87049092</td>\n",
       "      <td>Am J Emerg Med 8703; 4(6):511-3</td>\n",
       "      <td>Aged; Aged, 80 and over; Case Report; Female; ...</td>\n",
       "      <td>Nasogastric intubation: morbidity in an asympt...</td>\n",
       "      <td>JOURNAL ARTICLE.</td>\n",
       "      <td>An unusual case of a misdirected nasogastric t...</td>\n",
       "      <td>Gough D; Rust D.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sequential identifier  MEDLINE identifier  \\\n",
       "0                     1            87049087   \n",
       "1                     2            87049088   \n",
       "2                     3            87049089   \n",
       "3                     4            87049090   \n",
       "4                     5            87049092   \n",
       "\n",
       "                              source  \\\n",
       "0    Am J Emerg Med 8703; 4(6):491-5   \n",
       "1  Am J Emerg Med 8703; 4(6):496-500   \n",
       "2    Am J Emerg Med 8703; 4(6):501-3   \n",
       "3    Am J Emerg Med 8703; 4(6):504-6   \n",
       "4    Am J Emerg Med 8703; 4(6):511-3   \n",
       "\n",
       "                                          mesh_terms  \\\n",
       "0  Allied Health Personnel/*; Electric Countersho...   \n",
       "1  Antidepressive Agents, Tricyclic/*PO; Arrhythm...   \n",
       "2  Adult; Aircraft/*; Altitude/*; Blood Gas Monit...   \n",
       "3  Adolescence; Adult; Aged; Blood Glucose/*ME; D...   \n",
       "4  Aged; Aged, 80 and over; Case Report; Female; ...   \n",
       "\n",
       "                                               title  publication type  \\\n",
       "0  Refibrillation managed by EMT-Ds: incidence an...  JOURNAL ARTICLE.   \n",
       "1  Tricyclic antidepressant overdose: emergency d...  JOURNAL ARTICLE.   \n",
       "2  Transconjunctival oxygen monitoring as a predi...  JOURNAL ARTICLE.   \n",
       "3  Serum glucose changes after administration of ...  JOURNAL ARTICLE.   \n",
       "4  Nasogastric intubation: morbidity in an asympt...  JOURNAL ARTICLE.   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Some patients converted from ventricular fibri...   \n",
       "1  There is controversy regarding the appropriate...   \n",
       "2  As the use of helicopters for air transport of...   \n",
       "3  A prospective clinical trial was conducted to ...   \n",
       "4  An unusual case of a misdirected nasogastric t...   \n",
       "\n",
       "                                              author  is_relevant_ind  \\\n",
       "0                               Stults KR; Brown DD.              NaN   \n",
       "1                 Foulke GE; Albertson TE; Walby WF.              NaN   \n",
       "2  Shufflebarger C; Jehle D; Cottington E; Martin M.              NaN   \n",
       "3                                          Adler PM.              NaN   \n",
       "4                                   Gough D; Rust D.              NaN   \n",
       "\n",
       "   Relevance_total  \n",
       "0              NaN  \n",
       "1              NaN  \n",
       "2              NaN  \n",
       "3              NaN  \n",
       "4              NaN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "judged_df.rename(columns={\"Document-UI\": \"MEDLINE identifier\"}, inplace=True)\n",
    "\n",
    "# Converting key to string\n",
    "ohsumed_combined_df[\"MEDLINE identifier\"] = ohsumed_combined_df[\"MEDLINE identifier\"].astype(int)\n",
    "judged_df[\"MEDLINE identifier\"] = judged_df[\"MEDLINE identifier\"].astype(int)\n",
    "judged_df[\"is_relevant_ind\"] = 1\n",
    "\n",
    "# Perform the left join\n",
    "merged_df = ohsumed_combined_df.merge(judged_df, on=\"MEDLINE identifier\", how=\"left\")\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(f\"Resulting DataFrame shape: {merged_df.shape}\")\n",
    "print(merged_df.head())\n",
    "\n",
    "# Create a new column Relevance_total based on the rules that can be kept showing relevance for all three relevance columns\n",
    "merged_df[\"Relevance_total\"] = np.where(\n",
    "    ~merged_df[\"Relevance1\"].isna(),  # If Relevance1 is not NaN, take it\n",
    "    merged_df[\"Relevance1\"],\n",
    "    np.where(\n",
    "        ~merged_df[\"Relevance2\"].isna(),  # Else if Relevance2 is not NaN, take it\n",
    "        merged_df[\"Relevance2\"],\n",
    "        merged_df[\"Relevance3\"]  # Else take Relevance3\n",
    "    )\n",
    ")\n",
    "merged_df.head(10000)\n",
    "\n",
    "# Drop the specified columns\n",
    "columns_to_drop = [\"Query\", \"Document-Index\", \"Relevance1\", \"Relevance2\", \"Relevance3\"]\n",
    "merged_df.drop(columns=columns_to_drop, inplace=True)\n",
    "# Mapping relevance labels to int\n",
    "relevance_mapping = {'n': 0, 'p': 1, 'd': 2}\n",
    "\n",
    "# Filling mising values in abstract so that BERT can be trained on strings\n",
    "merged_df[\"abstract\"] = merged_df[\"abstract\"].fillna(\"\")\n",
    "\n",
    "# Apply the mapping to the Relevance1 column\n",
    "merged_df[\"Relevance_total\"] = merged_df[\"Relevance_total\"].map(relevance_mapping)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(f\"Resulting DataFrame shape: {merged_df.shape}\")\n",
    "merged_df.head()\n",
    "\n",
    "#filtered_df = merged_df[merged_df[\"Relevance1\"].notna()]\n",
    "#\n",
    "## Display the filtered DataFrame\n",
    "#print(f\"Number of rows where Relevance1 is not NaN: {len(filtered_df)}\")\n",
    "#print(filtered_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8661ed6849b71fb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T00:23:17.158143Z",
     "start_time": "2025-01-03T00:23:17.137846Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows where is_relevant_ind = 1: 14430\n"
     ]
    }
   ],
   "source": [
    "#Checking how many relevant documents are present in merged DF. Expectation: 16140\n",
    "count_is_relevant = merged_df[merged_df[\"is_relevant_ind\"] == 1].shape[0]\n",
    "print(f\"Number of rows where is_relevant_ind = 1: {count_is_relevant}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bf3be64eba304f",
   "metadata": {},
   "source": [
    "Now, we have a merged dataframe, containing all document data and also the relevance labeling of the documents. Now we can proceed with splitting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T00:23:17.434333Z",
     "start_time": "2025-01-03T00:23:17.216823Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set size: 14430\n",
      "Training set size: 284015\n",
      "Validation set size: 50121\n"
     ]
    }
   ],
   "source": [
    "test = merged_df[merged_df[\"is_relevant_ind\"] == 1]\n",
    "remaining_rows = merged_df[merged_df[\"is_relevant_ind\"] != 1]\n",
    "training, validation = train_test_split(remaining_rows, test_size=0.15, random_state=42)\n",
    "\n",
    "print(f\"Test set size: {len(test)}\")\n",
    "print(f\"Training set size: {len(training)}\")\n",
    "print(f\"Validation set size: {len(validation)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1384fb8e2299c55",
   "metadata": {},
   "source": [
    "Training BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7295d700bce05d7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T00:23:17.818083Z",
     "start_time": "2025-01-03T00:23:17.473316Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\timgr\\AppData\\Local\\Temp\\ipykernel_54960\\4000273643.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test.loc[:, 'title_abstract'] = test['title'] + ' ' + test['abstract']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sequential identifier</th>\n",
       "      <th>MEDLINE identifier</th>\n",
       "      <th>source</th>\n",
       "      <th>mesh_terms</th>\n",
       "      <th>title</th>\n",
       "      <th>publication type</th>\n",
       "      <th>abstract</th>\n",
       "      <th>author</th>\n",
       "      <th>is_relevant_ind</th>\n",
       "      <th>Relevance_total</th>\n",
       "      <th>title_abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33697</th>\n",
       "      <td>33698</td>\n",
       "      <td>87124329</td>\n",
       "      <td>Am Heart J 8705; 113(2 Pt 1):273-9</td>\n",
       "      <td>Aged; Comparative Study; Electrocardiography/*...</td>\n",
       "      <td>Non-Q wave myocardial infarction: recent chang...</td>\n",
       "      <td>JOURNAL ARTICLE.</td>\n",
       "      <td>A community-wide study of patients hospitalize...</td>\n",
       "      <td>Goldberg RJ; Gore JM; Alpert JS; Dalen JE.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Non-Q wave myocardial infarction: recent chang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245340</th>\n",
       "      <td>245341</td>\n",
       "      <td>90357619</td>\n",
       "      <td>Transplant Proc 9011; 22(4):1885-6</td>\n",
       "      <td>Antibodies, Anti-Idiotypic/*IM; Antibodies, Mo...</td>\n",
       "      <td>IgM-anti-IgG antibody as cause of positive B-c...</td>\n",
       "      <td>JOURNAL ARTICLE.</td>\n",
       "      <td></td>\n",
       "      <td>Terness P; Berteli AJ; Steinitz M; Mytillineos...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IgM-anti-IgG antibody as cause of positive B-c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306208</th>\n",
       "      <td>306209</td>\n",
       "      <td>91170056</td>\n",
       "      <td>J Appl Physiol 9106; 69(6):2091-6</td>\n",
       "      <td>Animal; Blood Pressure; Cardiac Output; Hemody...</td>\n",
       "      <td>Altered baroreflex function after tail suspens...</td>\n",
       "      <td>JOURNAL ARTICLE.</td>\n",
       "      <td>Experiments were performed on conscious chroni...</td>\n",
       "      <td>Brizzee BL; Walker BR.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Altered baroreflex function after tail suspens...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5057</th>\n",
       "      <td>5058</td>\n",
       "      <td>87097540</td>\n",
       "      <td>Am J Obstet Gynecol 8704; 156(1):52-6</td>\n",
       "      <td>Apgar Score; Cesarean Section/*; Delivery/*MT;...</td>\n",
       "      <td>Randomized management of the second nonvertex ...</td>\n",
       "      <td>JOURNAL ARTICLE.</td>\n",
       "      <td>Sixty twin deliveries after the thirty-fifth g...</td>\n",
       "      <td>Rabinovici J; Barkai G; Reichman B; Serr DM; M...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Randomized management of the second nonvertex ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120427</th>\n",
       "      <td>120428</td>\n",
       "      <td>88110706</td>\n",
       "      <td>Chest 8805; 93(2):294-8</td>\n",
       "      <td>Adult; Aged; Arteries/*; Bloodletting/*; Carbo...</td>\n",
       "      <td>Single arterial puncture vs arterial cannula f...</td>\n",
       "      <td>JOURNAL ARTICLE.</td>\n",
       "      <td>In an attempt to find the least invasive, safe...</td>\n",
       "      <td>Frye M; DiBenedetto R; Lain D; Morgan K.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Single arterial puncture vs arterial cannula f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sequential identifier  MEDLINE identifier  \\\n",
       "33697                  33698            87124329   \n",
       "245340                245341            90357619   \n",
       "306208                306209            91170056   \n",
       "5057                    5058            87097540   \n",
       "120427                120428            88110706   \n",
       "\n",
       "                                       source  \\\n",
       "33697      Am Heart J 8705; 113(2 Pt 1):273-9   \n",
       "245340     Transplant Proc 9011; 22(4):1885-6   \n",
       "306208      J Appl Physiol 9106; 69(6):2091-6   \n",
       "5057    Am J Obstet Gynecol 8704; 156(1):52-6   \n",
       "120427                Chest 8805; 93(2):294-8   \n",
       "\n",
       "                                               mesh_terms  \\\n",
       "33697   Aged; Comparative Study; Electrocardiography/*...   \n",
       "245340  Antibodies, Anti-Idiotypic/*IM; Antibodies, Mo...   \n",
       "306208  Animal; Blood Pressure; Cardiac Output; Hemody...   \n",
       "5057    Apgar Score; Cesarean Section/*; Delivery/*MT;...   \n",
       "120427  Adult; Aged; Arteries/*; Bloodletting/*; Carbo...   \n",
       "\n",
       "                                                    title  publication type  \\\n",
       "33697   Non-Q wave myocardial infarction: recent chang...  JOURNAL ARTICLE.   \n",
       "245340  IgM-anti-IgG antibody as cause of positive B-c...  JOURNAL ARTICLE.   \n",
       "306208  Altered baroreflex function after tail suspens...  JOURNAL ARTICLE.   \n",
       "5057    Randomized management of the second nonvertex ...  JOURNAL ARTICLE.   \n",
       "120427  Single arterial puncture vs arterial cannula f...  JOURNAL ARTICLE.   \n",
       "\n",
       "                                                 abstract  \\\n",
       "33697   A community-wide study of patients hospitalize...   \n",
       "245340                                                      \n",
       "306208  Experiments were performed on conscious chroni...   \n",
       "5057    Sixty twin deliveries after the thirty-fifth g...   \n",
       "120427  In an attempt to find the least invasive, safe...   \n",
       "\n",
       "                                                   author  is_relevant_ind  \\\n",
       "33697          Goldberg RJ; Gore JM; Alpert JS; Dalen JE.              NaN   \n",
       "245340  Terness P; Berteli AJ; Steinitz M; Mytillineos...              NaN   \n",
       "306208                             Brizzee BL; Walker BR.              NaN   \n",
       "5057    Rabinovici J; Barkai G; Reichman B; Serr DM; M...              NaN   \n",
       "120427           Frye M; DiBenedetto R; Lain D; Morgan K.              NaN   \n",
       "\n",
       "        Relevance_total                                     title_abstract  \n",
       "33697               NaN  Non-Q wave myocardial infarction: recent chang...  \n",
       "245340              NaN  IgM-anti-IgG antibody as cause of positive B-c...  \n",
       "306208              NaN  Altered baroreflex function after tail suspens...  \n",
       "5057                NaN  Randomized management of the second nonvertex ...  \n",
       "120427              NaN  Single arterial puncture vs arterial cannula f...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.loc[:, 'title_abstract'] = training['title'] + ' ' + training['abstract']\n",
    "test.loc[:, 'title_abstract'] = test['title'] + ' ' + test['abstract']\n",
    "validation.loc[:, 'title_abstract'] = validation['title'] + ' ' + validation['abstract']\n",
    "training.head()\n",
    "test.head()\n",
    "validation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f67bf21",
   "metadata": {},
   "source": [
    "to assign the right \"ground truth\" labels, we have to find the correct mesh terms and parse our files to find sensitive content. The Terms can be found in mtrees2019.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "966286ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\timgr\\AppData\\Local\\Temp\\ipykernel_54960\\3878842028.py:64: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, \"processed_mesh_terms\"] = df[\"mesh_terms\"].apply(preprocess_mesh_terms)\n",
      "C:\\Users\\timgr\\AppData\\Local\\Temp\\ipykernel_54960\\3878842028.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, \"sensitive\"] = df[\"processed_mesh_terms\"].apply(is_sensitive_regex)\n",
      "C:\\Users\\timgr\\AppData\\Local\\Temp\\ipykernel_54960\\3878842028.py:66: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.loc[:, \"label\"] = df[\"sensitive\"]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset: 8.1%\n",
      "Judged documents: 12.3%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\timgr\\AppData\\Local\\Temp\\ipykernel_54960\\3878842028.py:79: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.drop(columns=[\"sensitive\"], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Parse the .bin file to extract sensitive MeSH terms\n",
    "def parse_mesh_bin(file_path, target_categories):\n",
    "    \"\"\"\n",
    "    Extract MeSH terms under specific categories from a .bin file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the .bin file.\n",
    "        target_categories (list): List of categories (e.g., [\"C12\", \"C13\"]).\n",
    "\n",
    "    Returns:\n",
    "        list: List of MeSH terms under the target categories.\n",
    "    \"\"\"\n",
    "    mesh_terms = []\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            # Split by ';' to separate terms and categories\n",
    "            parts = line.strip().split(\";\")\n",
    "            if len(parts) > 1:\n",
    "                term, category = parts[0].strip().lower(), parts[1].strip()\n",
    "                # Include terms under target categories\n",
    "                if any(category.startswith(target) for target in target_categories):\n",
    "                    mesh_terms.append(term)\n",
    "    return mesh_terms\n",
    "\n",
    "# Path to the .bin file\n",
    "file_path = \"data/mtrees2019.bin\"\n",
    "\n",
    "# Extract MeSH terms under C12 and C13\n",
    "sensitive_terms = parse_mesh_bin(file_path, [\"C12\", \"C13\"])\n",
    "\n",
    "# Step 2: Preprocessing function for the `mesh_terms` column\n",
    "def preprocess_mesh_terms(mesh_terms):\n",
    "    \"\"\"\n",
    "    Normalize and preprocess the MeSH terms in a document.\n",
    "\n",
    "    Args:\n",
    "        mesh_terms (str): The raw MeSH terms for a document.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of cleaned MeSH terms.\n",
    "    \"\"\"\n",
    "    if isinstance(mesh_terms, str):\n",
    "        terms = mesh_terms.split(\";\")\n",
    "        return [re.sub(r\"/.*\", \"\", term).strip().lower() for term in terms]\n",
    "    return []\n",
    "\n",
    "# Step 3: Define the matching function using sensitive terms\n",
    "sensitive_pattern = re.compile(r\"\\b(\" + \"|\".join(re.escape(term) for term in sensitive_terms) + r\")\\b\", re.IGNORECASE)\n",
    "\n",
    "def is_sensitive_regex(terms):\n",
    "    \"\"\"\n",
    "    Check if any term in the document is sensitive based on MeSH terms.\n",
    "\n",
    "    Args:\n",
    "        terms (list of str): Processed MeSH terms.\n",
    "\n",
    "    Returns:\n",
    "        int: 1 if sensitive, 0 otherwise.\n",
    "    \"\"\"\n",
    "    return 1 if any(sensitive_pattern.search(term) for term in terms) else 0\n",
    "\n",
    "# Step 4: Apply preprocessing and matching to datasets\n",
    "for df in [training, validation, test]:\n",
    "    df.loc[:, \"processed_mesh_terms\"] = df[\"mesh_terms\"].apply(preprocess_mesh_terms)\n",
    "    df.loc[:, \"sensitive\"] = df[\"processed_mesh_terms\"].apply(is_sensitive_regex)\n",
    "    df.loc[:, \"label\"] = df[\"sensitive\"]\n",
    "\n",
    "# Step 5: Calculate sensitive document percentages\n",
    "datasets = [\n",
    "    (pd.concat([training, validation, test]), \"Full dataset\"),\n",
    "    (test, \"Judged documents\")\n",
    "]\n",
    "\n",
    "for df, name in datasets:\n",
    "    percentage = df[\"label\"].mean() * 100\n",
    "    print(f\"{name}: {percentage:.1f}%\")\n",
    "\n",
    "for df in [training, validation, test]:\n",
    "    df.drop(columns=[\"sensitive\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "840c6972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sensitive documents in Training:\n",
      "                                               mesh_terms  \\\n",
      "54170   Adult; Cardiovascular Diseases/ET/*MO; Diabete...   \n",
      "3827    Antigens, Bacterial/*AN; Chlamydia trachomatis...   \n",
      "63723   Adult; Evaluation Studies; Female; Hospitals, ...   \n",
      "144305  Adult; Antineoplastic Agents, Combined/*TU; Ca...   \n",
      "160573  Acidosis, Renal Tubular/*CO/ET; Adult; Case Re...   \n",
      "\n",
      "                                     processed_mesh_terms  label  \n",
      "54170   [adult, cardiovascular diseases, diabetes mell...      1  \n",
      "3827    [antigens, bacterial, chlamydia trachomatis, c...      1  \n",
      "63723   [adult, evaluation studies, female, hospitals,...      1  \n",
      "144305  [adult, antineoplastic agents, combined, case ...      1  \n",
      "160573  [acidosis, renal tubular, adult, case report, ...      1  \n",
      "Total sensitive documents in Training: 22371\n",
      "Relative amount of sensitive documents in Training: 7.9%\n",
      "\n",
      "Sensitive documents in Validation:\n",
      "                                               mesh_terms  \\\n",
      "129141  Electrocoagulation/AE/*IS/MT; Female; Follow-U...   \n",
      "170093  Adult; Child; Child, Preschool; Human; Hypospa...   \n",
      "174428  Dimercaptosuccinic Acid/*DU; Human; Organometa...   \n",
      "229509  Cohort Studies; Female; Human; Hypertension/CO...   \n",
      "202123  Adolescence; Adult; Catecholamines/*BL; Child;...   \n",
      "\n",
      "                                     processed_mesh_terms  label  \n",
      "129141  [electrocoagulation, female, follow-up studies...      1  \n",
      "170093  [adult, child, child, preschool, human, hyposp...      1  \n",
      "174428  [dimercaptosuccinic acid, human, organometalli...      1  \n",
      "229509  [cohort studies, female, human, hypertension, ...      1  \n",
      "202123  [adolescence, adult, catecholamines, child, ch...      1  \n",
      "Total sensitive documents in Validation: 4007\n",
      "Relative amount of sensitive documents in Validation: 8.0%\n",
      "\n",
      "Sensitive documents in Test:\n",
      "                                             mesh_terms  \\\n",
      "153   Female; Human; Pregnancy; Propranolol/TU; Puer...   \n",
      "222   Actinomycosis/*CO; Adult; Case Report; Human; ...   \n",
      "625   Adult; Analysis of Variance; Bromocriptine/AD/...   \n",
      "1362  Adult; Case Report; Disseminated Intravascular...   \n",
      "1504  Adult; Aged; Biopsy; Blood Platelets/*ME; Bloo...   \n",
      "\n",
      "                                   processed_mesh_terms  label  \n",
      "153   [female, human, pregnancy, propranolol, puerpe...      1  \n",
      "222   [actinomycosis, adult, case report, human, kid...      1  \n",
      "625   [adult, analysis of variance, bromocriptine, c...      1  \n",
      "1362  [adult, case report, disseminated intravascula...      1  \n",
      "1504  [adult, aged, biopsy, blood platelets, blood p...      1  \n",
      "Total sensitive documents in Test: 1781\n",
      "Relative amount of sensitive documents in Test: 12.3%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for df, name in [(training, \"Training\"), (validation, \"Validation\"), (test, \"Test\")]:\n",
    "    sensitive_docs = df[df[\"label\"] == 1]\n",
    "    total_docs = len(df)\n",
    "    relative_percentage = (len(sensitive_docs) / total_docs) * 100\n",
    "    \n",
    "    print(f\"Sensitive documents in {name}:\")\n",
    "    print(sensitive_docs[[\"mesh_terms\", \"processed_mesh_terms\", \"label\"]].head())\n",
    "    print(f\"Total sensitive documents in {name}: {len(sensitive_docs)}\")\n",
    "    print(f\"Relative amount of sensitive documents in {name}: {relative_percentage:.1f}%\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60b3fe403a111283",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T00:23:25.188858Z",
     "start_time": "2025-01-03T00:23:18.525960Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "train_dataset = Dataset.from_pandas(training)\n",
    "test_dataset = Dataset.from_pandas(test)\n",
    "validation_dataset = Dataset.from_pandas(validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b333bd941a53a20b",
   "metadata": {},
   "source": [
    "Tokenizing sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2375df8226a5ec6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T01:02:21.211707Z",
     "start_time": "2025-01-03T00:23:25.250845Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d264b6fe479d44159d4ec578c22dfa8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/284015 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf04c6a59c9b44c1b316a85224ef1679",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14430 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "222676b1982e49f48899cb96991ebe49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50121 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Tokenize the dataset for BERTs training\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"title_abstract\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "validation_dataset = validation_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "train_dataset = train_dataset.remove_columns([\"title_abstract\"])\n",
    "test_dataset = test_dataset.remove_columns([\"title_abstract\"])\n",
    "validation_dataset = validation_dataset.remove_columns([\"title_abstract\"])\n",
    "\n",
    "train_dataset.set_format(\"torch\")\n",
    "test_dataset.set_format(\"torch\")\n",
    "validation_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a8d2480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: False\n",
      "CUDA version: None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"CUDA version:\", torch.version.cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32804bf6341b85c",
   "metadata": {},
   "source": [
    "Loading DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac2832a16280a76b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-03T01:03:01.842828Z",
     "start_time": "2025-01-03T01:03:01.830931Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtraining\u001b[49m\u001b[38;5;241m.\u001b[39mcolumns)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'training' is not defined"
     ]
    }
   ],
   "source": [
    "print(training.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c7ef9f777cd8f50",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-01-03T01:03:07.234154Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using CPU\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'training' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 48\u001b[0m\n\u001b[0;32m     45\u001b[0m check_gpu_availability()  \n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Load the model with GPU support \u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m num_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(\u001b[43mtraining\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m])) \n\u001b[0;32m     49\u001b[0m model \u001b[38;5;241m=\u001b[39m DistilBertForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(     \n\u001b[0;32m     50\u001b[0m    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistilbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m,     \n\u001b[0;32m     51\u001b[0m    num_labels\u001b[38;5;241m=\u001b[39mnum_labels \n\u001b[0;32m     52\u001b[0m )\u001b[38;5;241m.\u001b[39mcuda()  \n\u001b[0;32m     54\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(     \n\u001b[0;32m     55\u001b[0m    output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./results\u001b[39m\u001b[38;5;124m\"\u001b[39m,     \n\u001b[0;32m     56\u001b[0m    evaluation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,     \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     62\u001b[0m    metric_for_best_model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m\"\u001b[39m \n\u001b[0;32m     63\u001b[0m )  \n",
      "\u001b[1;31mNameError\u001b[0m: name 'training' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments, set_seed\n",
    "import random\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, fbeta_score \n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "#setting seeds\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "set_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# Add GPU check function \n",
    "def check_gpu_availability():     \n",
    "   if torch.cuda.is_available():         \n",
    "       print(f\"GPU available: {torch.cuda.get_device_name(0)}\")         \n",
    "       print(f\"Total GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")         \n",
    "       print(f\"Available GPU memory: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")     \n",
    "   else:         \n",
    "       print(\"No GPU available, using CPU\")  \n",
    "\n",
    "# Define the compute_metrics function - FIXED VERSION with F2 \n",
    "def compute_metrics(eval_pred):     \n",
    "   logits, labels = eval_pred     \n",
    "   predictions = logits.argmax(axis=-1)          \n",
    "   precision = precision_score(labels, predictions, average=\"binary\")     \n",
    "   recall = recall_score(labels, predictions, average=\"binary\")     \n",
    "   f1 = f1_score(labels, predictions, average=\"binary\")     \n",
    "   f2 = fbeta_score(labels, predictions, beta=2, average=\"binary\")     \n",
    "   accuracy = accuracy_score(labels, predictions)          \n",
    "   \n",
    "   return {         \n",
    "       \"accuracy\": accuracy,         \n",
    "       \"precision\": precision,         \n",
    "       \"recall\": recall,         \n",
    "       \"f1\": f1,         \n",
    "       \"f2\": f2     \n",
    "   }  \n",
    "\n",
    "check_gpu_availability()  \n",
    "\n",
    "# Load the model with GPU support \n",
    "num_labels = len(set(training[\"label\"])) \n",
    "model = DistilBertForSequenceClassification.from_pretrained(     \n",
    "   \"distilbert-base-uncased\",     \n",
    "   num_labels=num_labels \n",
    ").cuda()  \n",
    "\n",
    "training_args = TrainingArguments(     \n",
    "   output_dir=\"./results\",     \n",
    "   evaluation_strategy=\"epoch\",     \n",
    "   no_cuda=False,      # Enable GPU     \n",
    "   fp16=True,         # Enable mixed precision training     \n",
    "   logging_dir=\"./logs\",     \n",
    "   logging_steps=10,     \n",
    "   save_strategy=\"epoch\",     \n",
    "   metric_for_best_model=\"f1\" \n",
    ")  \n",
    "\n",
    "trainer = Trainer(     \n",
    "   model=model,     \n",
    "   args=training_args,     \n",
    "   train_dataset=train_dataset,     \n",
    "   eval_dataset=validation_dataset,     \n",
    "   tokenizer=tokenizer,     \n",
    "   compute_metrics=compute_metrics \n",
    ")  \n",
    "\n",
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Get validation predictions for threshold optimization\n",
    "print(\"Getting validation predictions for threshold optimization...\")\n",
    "validation_output = trainer.predict(validation_dataset)\n",
    "validation_logits = validation_output.predictions\n",
    "validation_labels = validation_output.label_ids\n",
    "\n",
    "# Convert logits to probabilities (on GPU)\n",
    "validation_probabilities = torch.nn.functional.softmax(torch.tensor(validation_logits).cuda(), dim=-1)[:, 1].cpu().numpy()\n",
    "\n",
    "# Find optimal threshold\n",
    "print(\"Finding optimal threshold...\")\n",
    "thresholds = np.arange(0, 1.01, 0.01)\n",
    "best_threshold = 0\n",
    "best_f1 = 0\n",
    "\n",
    "for threshold in thresholds:\n",
    "   predictions = (validation_probabilities >= threshold).astype(int)\n",
    "   f1 = f1_score(validation_labels, predictions)\n",
    "   if f1 > best_f1:\n",
    "       best_f1 = f1\n",
    "       best_threshold = threshold\n",
    "\n",
    "print(f\"Best Threshold: {best_threshold}, Best F1-Score: {best_f1}\")\n",
    "\n",
    "# Apply best threshold to test set\n",
    "print(\"Evaluating test set with optimal threshold...\")\n",
    "test_output = trainer.predict(test_dataset)\n",
    "test_logits = test_output.predictions\n",
    "test_labels = test_output.label_ids\n",
    "test_probabilities = torch.nn.functional.softmax(torch.tensor(test_logits).cuda(), dim=-1)[:, 1].cpu().numpy()\n",
    "test_predictions = (test_probabilities >= best_threshold).astype(int)\n",
    "\n",
    "# Calculate and print final metrics\n",
    "final_metrics = {\n",
    "   \"accuracy\": accuracy_score(test_labels, test_predictions),\n",
    "   \"precision\": precision_score(test_labels, test_predictions),\n",
    "   \"recall\": recall_score(test_labels, test_predictions),\n",
    "   \"f1\": f1_score(test_labels, test_predictions),\n",
    "   \"f2\": fbeta_score(test_labels, test_predictions, beta=2)\n",
    "}\n",
    "\n",
    "\n",
    "test_df = pd.DataFrame(test_dataset)\n",
    "\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    \"sequential identifier\": test_df[\"sequential identifier\"],  \n",
    "    \"title_abstract\": test_df[\"title\"],  \n",
    "    \"actual_sensitivity\": test_labels,\n",
    "    \"predicted_sensitivity\": test_predictions\n",
    "})\n",
    "\n",
    "# Save to CSV\n",
    "results.to_csv(\"sensitivity_predictions_comparison.csv\", index=False)\n",
    "\n",
    "print(\"\\nFinal Test Metrics with optimized threshold:\")\n",
    "for metric, value in final_metrics.items():\n",
    "   print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaa73cb",
   "metadata": {},
   "source": [
    "## Statistical Significance for these results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
